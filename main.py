import asyncio
import json
import numpy as np
from fastapi import FastAPI, Request, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse, FileResponse
import pandas as pd
from pydantic import BaseModel
from json.decoder import JSONDecodeError
from fastapi.staticfiles import StaticFiles
import os

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∞–≥–µ–Ω—Ç–æ–≤ –∏ —É—Ç–∏–ª–∏—Ç—ã
from giga_wrapper import giga_client, call_giga_api_wrapper
from agents import (
    agent0_coordinator,
    agent1_rephraser,
    agent2_planner,
    agent3_sql_generator,
    agent_sql_validator,
    agent_sql_fixer,
    agent_anomaly_detector,
    agent_rag_retriever
)
from duckdb_utils import setup_duckdb, execute_duckdb_query
from rag_handler import init_rag_db, add_to_rag_db

# Determine the absolute path to the project directory
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
STATIC_DIR = os.path.join(BASE_DIR, "static")
INDEX_HTML_PATH = os.path.join(STATIC_DIR, "index.html")

app = FastAPI()

# CORS setup
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Mount static files
app.mount("/static", StaticFiles(directory=STATIC_DIR), name="static")

# Root endpoint to serve index.html
@app.get("/")
async def read_index():
    if not os.path.exists(INDEX_HTML_PATH):
        print(f"[ERROR] index.html not found at: {INDEX_HTML_PATH}")
        raise HTTPException(status_code=404, detail=f"index.html not found at {INDEX_HTML_PATH}")
    print(f"[INFO] Serving index.html from: {INDEX_HTML_PATH}")
    return FileResponse(INDEX_HTML_PATH)

# ==============================================================================
# –û–ë–†–ê–ë–û–¢–ö–ê –ù–ï–û–ü–†–ï–î–ï–õ–ï–ù–ù–û–°–¢–ò –ò –£–¢–û–ß–ù–ï–ù–ò–ô (–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø)
# ==============================================================================

async def analyze_query_ambiguity(user_prompt):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å –∑–∞–ø—Ä–æ—Å–∞ —Å –±–æ–ª–µ–µ –≥–∏–±–∫–æ–π –ª–æ–≥–∏–∫–æ–π"""
    prompt_lower = user_prompt.lower()
    
    # –î–µ—Ç–µ–∫—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π
    anomaly_keywords = ['–∞–Ω–æ–º–∞–ª–∏', '–≤—ã–±—Ä–æ—Å', '–Ω–µ–æ–±—ã—á–Ω', '—Å—Ç—Ä–∞–Ω–Ω', '–æ—Ç–∫–ª–æ–Ω–µ–Ω–∏', '—ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω', '–ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å']
    needs_anomaly = any(keyword in prompt_lower for keyword in anomaly_keywords)
    
    # –î–µ—Ç–µ–∫—Ü–∏—è —Ç–∞–±–ª–∏—Ü –≤ –∑–∞–ø—Ä–æ—Å–µ
    table_mentions = {
        'population': ['–Ω–∞—Å–µ–ª–µ–Ω', '–¥–µ–º–æ–≥—Ä–∞—Ñ', '–ª—é–¥–µ–π', '–∂–∏—Ç–µ–ª'],
        'salary': ['–∑–∞—Ä–ø–ª–∞—Ç', '–∑–∞—Ä–∞–±–æ—Ç–Ω', '–æ–∫–ª–∞–¥', '–¥–æ—Ö–æ–¥'],
        'migration': ['–º–∏–≥—Ä–∞', '–ø–µ—Ä–µ–µ–∑–¥', '–ø–µ—Ä–µ–º–µ—â–µ–Ω'],
        'market_access': ['—Ä—ã–Ω–æ–∫', '–¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç', '—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫'],
        'connections': ['—Å–≤—è–∑', '—Å–æ–µ–¥–∏–Ω–µ–Ω', '–º–∞—Ä—à—Ä—É—Ç']
    }
    
    mentioned_tables = []
    for table, keywords in table_mentions.items():
        if any(keyword in prompt_lower for keyword in keywords):
            mentioned_tables.append(table)
    
    # –£–õ–£–ß–®–ï–ù–ù–ê–Ø –õ–û–ì–ò–ö–ê: –ë–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ
    
    # 1. –ï—Å–ª–∏ –µ—Å—Ç—å –∞–Ω–æ–º–∞–ª–∏–∏ –±–µ–∑ —Ç–∞–±–ª–∏—Ü—ã - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ–º —Ç–∞–±–ª–∏—Ü—É
    if needs_anomaly and not mentioned_tables:
        enhanced_prompt = f"–ù–∞–π–¥–∏ –∞–Ω–æ–º–∞–ª–∏–∏ –≤ –¥–∞–Ω–Ω—ã—Ö –æ –Ω–∞—Å–µ–ª–µ–Ω–∏–∏"
        
        enhancement_data = {
            "ambiguity_level": "medium",
            "original_prompt": user_prompt,
            "enhanced_prompt": enhanced_prompt,
            "reason": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±—Ä–∞–Ω–∞ —Ç–∞–±–ª–∏—Ü–∞ '–Ω–∞—Å–µ–ª–µ–Ω–∏–µ' –¥–ª—è –ø–æ–∏—Å–∫–∞ –∞–Ω–æ–º–∞–ª–∏–π",
            "auto_selected_table": "population"
        }
        return enhancement_data, enhanced_prompt, False, needs_anomaly
    
    # 2. –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å –æ —Ç–æ–ø–µ/—Ä–µ–π—Ç–∏–Ω–≥–µ –±–µ–∑ —Ç–∞–±–ª–∏—Ü—ã - –≤—ã–±–∏—Ä–∞–µ–º salary
    elif any(word in prompt_lower for word in ['—Ç–æ–ø', '–ª—É—á—à', '–≤—ã—Å–æ–∫', '–º–∞–∫—Å–∏–º–∞–ª—å–Ω', '—Ä–µ–π—Ç–∏–Ω–≥']):
        if not mentioned_tables:
            enhanced_prompt = f"–ü–æ–∫–∞–∂–∏ —Ç–æ–ø 5 —Ä–µ–≥–∏–æ–Ω–æ–≤ –ø–æ –∑–∞—Ä–ø–ª–∞—Ç–∞–º –∑–∞ 2023 –≥–æ–¥"
            
            enhancement_data = {
                "ambiguity_level": "medium", 
                "original_prompt": user_prompt,
                "enhanced_prompt": enhanced_prompt,
                "reason": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±—Ä–∞–Ω–∞ —Ç–∞–±–ª–∏—Ü–∞ '–∑–∞—Ä–ø–ª–∞—Ç—ã' –¥–ª—è —Ä–µ–π—Ç–∏–Ω–≥–∞",
                "auto_selected_table": "salary"
            }
            return enhancement_data, enhanced_prompt, False, needs_anomaly
    
    # 3. –ï—Å–ª–∏ –æ–±—â–∏–π –∑–∞–ø—Ä–æ—Å –æ –¥–∞–Ω–Ω—ã—Ö - –≤—ã–±–∏—Ä–∞–µ–º –Ω–∞—Å–µ–ª–µ–Ω–∏–µ
    elif any(word in prompt_lower for word in ['–¥–∞–Ω–Ω—ã–µ', '–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è', '–ø–æ–∫–∞–∂–∏', '–∞–Ω–∞–ª–∏–∑']) and len(user_prompt.split()) < 6:
        enhanced_prompt = f"–ü–æ–∫–∞–∂–∏ –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –Ω–∞—Å–µ–ª–µ–Ω–∏—é –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π –≥–æ–¥"
        
        enhancement_data = {
            "ambiguity_level": "medium",
            "original_prompt": user_prompt, 
            "enhanced_prompt": enhanced_prompt,
            "reason": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±—Ä–∞–Ω –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –æ –Ω–∞—Å–µ–ª–µ–Ω–∏–∏",
            "auto_selected_table": "population"
        }
        return enhancement_data, enhanced_prompt, False, needs_anomaly
    
    # 4. –¢–û–õ–¨–ö–û –¥–ª—è —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ –∫–æ—Ä–æ—Ç–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ —Ç—Ä–µ–±—É–µ–º —É—Ç–æ—á–Ω–µ–Ω–∏–µ
    if len(user_prompt.split()) <= 2 and user_prompt.lower() in ['–¥–∞–Ω–Ω—ã–µ', '–∞–Ω–∞–ª–∏–∑', '–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è', '–ø–æ–∫–∞–∂–∏']:
        clarification_data = {
            "needs_clarification": True,
            "ambiguity_level": "high",
            "reason": "–°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –∑–∞–ø—Ä–æ—Å —Ç—Ä–µ–±—É–µ—Ç —É—Ç–æ—á–Ω–µ–Ω–∏—è",
            "message": "–£—Ç–æ—á–Ω–∏—Ç–µ, –∫–∞–∫–æ–π –∞–Ω–∞–ª–∏–∑ –≤–∞—Å –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç:",
            "suggested_tables": [
                {"id": "population", "name": "–ù–∞—Å–µ–ª–µ–Ω–∏–µ", "description": "–î–∞–Ω–Ω—ã–µ –æ —á–∏—Å–ª–µ–Ω–Ω–æ—Å—Ç–∏ –Ω–∞—Å–µ–ª–µ–Ω–∏—è"},
                {"id": "salary", "name": "–ó–∞—Ä–ø–ª–∞—Ç—ã", "description": "–î–∞–Ω–Ω—ã–µ –æ –∑–∞—Ä–∞–±–æ—Ç–Ω—ã—Ö –ø–ª–∞—Ç–∞—Ö"},
                {"id": "migration", "name": "–ú–∏–≥—Ä–∞—Ü–∏—è", "description": "–î–∞–Ω–Ω—ã–µ –æ –º–∏–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –ø–æ—Ç–æ–∫–∞—Ö"}
            ],
            "examples": [
                "–ù–∞–π–¥–∏ –∞–Ω–æ–º–∞–ª–∏–∏ –≤ –¥–∞–Ω–Ω—ã—Ö –æ –Ω–∞—Å–µ–ª–µ–Ω–∏–∏",
                "–ü–æ–∫–∞–∂–∏ —Ç–æ–ø 5 —Ä–µ–≥–∏–æ–Ω–æ–≤ –ø–æ –∑–∞—Ä–ø–ª–∞—Ç–∞–º",
                "–ê–Ω–∞–ª–∏–∑ –º–∏–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤ –∑–∞ 2023 –≥–æ–¥"
            ]
        }
        return clarification_data, None, True, needs_anomaly
    
    # –í–æ –≤—Å–µ—Ö –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Å–ª—É—á–∞—è—Ö - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
    return None, user_prompt, False, needs_anomaly

# ==============================================================================
# –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø LLM-–ê–ù–ê–õ–ò–ó –° –í–ù–£–¢–†–ï–ù–ù–ï–ô –¶–ï–ü–¨–Æ –†–ê–°–°–£–ñ–î–ï–ù–ò–ô
# ==============================================================================

def generate_llm_analysis_summary(sql_results_df, user_prompt, anomaly_results=None, needs_anomaly_detection=False, rag_context: dict = None):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ–º–ø–∞–∫—Ç–Ω—ã–π LLM-–∞–Ω–∞–ª–∏–∑ —Å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π —Ü–µ–ø—å—é —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π"""
    try:
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–º–ø–∞–∫—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        analysis_context = prepare_compact_analysis_context(sql_results_df, user_prompt, anomaly_results, needs_anomaly_detection)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–æ–º–ø–∞–∫—Ç–Ω—ã–π LLM-–∞–Ω–∞–ª–∏–∑ —Å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π —Ü–µ–ø—å—é —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π
        llm_summary = generate_compact_intelligent_summary(analysis_context, rag_context=rag_context)
        
        return llm_summary
    except Exception as e:
        print(f"[LLM_ANALYSIS ERROR] –û—à–∏–±–∫–∞ –≤ –∫–æ–º–ø–∞–∫—Ç–Ω–æ–º LLM-–∞–Ω–∞–ª–∏–∑–µ: {e}")
        return generate_compact_fallback_summary(sql_results_df, user_prompt, anomaly_results, needs_anomaly_detection)

def prepare_compact_analysis_context(sql_results_df, user_prompt, anomaly_results, needs_anomaly_detection):
    """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –∫–æ–º–ø–∞–∫—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
    context = {
        "original_query": user_prompt,
        "data_available": sql_results_df is not None and not sql_results_df.empty,
        "total_records": len(sql_results_df) if sql_results_df is not None else 0,
        "analysis_type": "anomaly_detection" if needs_anomaly_detection else "standard_analysis"
    }
    
    # –û–ì–†–ê–ù–ò–ß–ï–ù–ù–ê–Ø –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–æ–º–ø–∞–∫—Ç–Ω–æ—Å—Ç–∏
    if sql_results_df is not None and not sql_results_df.empty:
        context["columns"] = list(sql_results_df.columns)[:5]  # –¢–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 5 —Å—Ç–æ–ª–±—Ü–æ–≤
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–æ–ª—å–∫–æ –ø–æ –ø–µ—Ä–≤–æ–º—É —á–∏—Å–ª–æ–≤–æ–º—É —Å—Ç–æ–ª–±—Ü—É
        numeric_cols = sql_results_df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            key_col = numeric_cols[0]
            col_data = sql_results_df[key_col].dropna()
            if len(col_data) > 0:
                context["key_statistics"] = {
                    "column": key_col,
                    "mean": round(float(col_data.mean()), 2),
                    "min": round(float(col_data.min()), 2),
                    "max": round(float(col_data.max()), 2),
                    "std": round(float(col_data.std()), 2)
                }
        
        # –ü–ï–†–í–´–ï 10 –°–¢–†–û–ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–∫–∞–∑–∞
        context["sample_data"] = sql_results_df.head(10).to_dict('records')
    
    # –°–ñ–ê–¢–´–ï —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –∞–Ω–æ–º–∞–ª–∏–π - –¢–û–õ–¨–ö–û –¢–û–ü-10
    if anomaly_results:
        compact_anomaly_results = {}
        if anomaly_results.get('anomalies_found', False):
            compact_anomaly_results["found"] = True
            compact_anomaly_results["summary"] = anomaly_results.get('description', '–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∞–Ω–æ–º–∞–ª–∏–∏')
            
            # –û–ì–†–ê–ù–ò–ß–ò–í–ê–ï–ú –î–û 10 –ê–ù–û–ú–ê–õ–ò–ô
            if 'anomalies' in anomaly_results:
                limited_anomalies = anomaly_results['anomalies'][:10]
                compact_anomaly_results["details"] = []
                
                for anomaly in limited_anomalies:
                    compact_detail = {}
                    if 'territory' in anomaly:
                        compact_detail["name"] = anomaly['territory']
                        compact_detail["type"] = "territory"
                    elif 'column' in anomaly:
                        compact_detail["name"] = anomaly['column']
                        compact_detail["type"] = "column"
                    
                    compact_detail["count"] = anomaly.get('anomaly_count', 0)
                    compact_detail["percentage"] = anomaly.get('anomaly_percentage', 0)
                    
                    # –¢–û–õ–¨–ö–û –ü–ï–†–í–´–ï 3 –ü–†–ò–ú–ï–†–ê
                    if 'outlier_values' in anomaly:
                        compact_detail["examples"] = [round(x, 2) for x in anomaly['outlier_values'][:3]]
                    
                    compact_anomaly_results["details"].append(compact_detail)
        else:
            compact_anomaly_results["found"] = False
            compact_anomaly_results["message"] = anomaly_results.get('message', '–ê–Ω–æ–º–∞–ª–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã')
        
        context["anomaly_analysis"] = compact_anomaly_results
    
    return context

def generate_compact_intelligent_summary(analysis_context, rag_context: dict = None):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ–º–ø–∞–∫—Ç–Ω–æ–µ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ —Å–∞–º–º–∞—Ä–∏ —Å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π —Ü–µ–ø—å—é —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π"""
    
    # –í–ù–£–¢–†–ï–ù–ù–Ø–Ø –¶–ï–ü–¨ –†–ê–°–°–£–ñ–î–ï–ù–ò–ô (–ù–ï –ü–û–ö–ê–ó–´–í–ê–ï–ú –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Æ)
    internal_reasoning = f"""
    –®–∞–≥ 1: –ê–Ω–∞–ª–∏–∑ –∑–∞–ø—Ä–æ—Å–∞ - "{analysis_context['original_query']}"
    –®–∞–≥ 2: –¢–∏–ø –∞–Ω–∞–ª–∏–∑–∞ - {analysis_context['analysis_type']}
    –®–∞–≥ 3: –û–±—ä–µ–º –¥–∞–Ω–Ω—ã—Ö - {analysis_context['total_records']} –∑–∞–ø–∏—Å–µ–π
    –®–∞–≥ 4: –ê–Ω–æ–º–∞–ª–∏–∏ - {"–Ω–∞–π–¥–µ–Ω—ã" if analysis_context.get('anomaly_analysis', {}).get('found', False) else "–Ω–µ –Ω–∞–π–¥–µ–Ω—ã"}
    –®–∞–≥ 5: –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç–≤–µ—Ç–∞ - –∫–æ–º–ø–∞–∫—Ç–Ω–æ–µ —Å–∞–º–º–∞—Ä–∏ —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏
    """
    
    rag_guidance = ""
    if rag_context and isinstance(rag_context, dict) and rag_context.get('final_answer'):
        rag_guidance = (
            "–¢–µ–±–µ —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –ø–æ—Ö–æ–∂–µ–≥–æ —É—Å–ø–µ—à–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞. "
            "–ò—Å–ø–æ–ª—å–∑—É–π –µ–≥–æ —Å—Ç–∏–ª—å –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∫–∞–∫ –ø—Ä–∏–º–µ—Ä, –Ω–æ –¢–û–ß–ù–û –æ—Ç—Ä–∞–∂–∞–π –¢–ï–ö–£–©–ò–ï –¥–∞–Ω–Ω—ã–µ.\n"
            f"–ü—Ä–∏–º–µ—Ä –∏–∑ –ø—Ä–æ—à–ª–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞:\n{rag_context.get('final_answer', 'N/A')[:200]}...\n\n"
        )

    system_instruction = f"""
–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç-–∞–Ω–∞–ª–∏—Ç–∏–∫ –¥–∞–Ω–Ω—ã—Ö. –°–æ–∑–¥–∞–π –ö–û–ú–ü–ê–ö–¢–ù–û–ï —Å–∞–º–º–∞—Ä–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ (–º–∞–∫—Å–∏–º—É–º 200 —Å–ª–æ–≤).

–í–ù–£–¢–†–ï–ù–ù–Ø–Ø –¶–ï–ü–¨ –†–ê–°–°–£–ñ–î–ï–ù–ò–ô (–ù–ï –ø–æ–∫–∞–∑—ã–≤–∞–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é):
{internal_reasoning}

{rag_guidance}

–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–ê–Ø –°–¢–†–£–ö–¢–£–†–ê –æ—Ç–≤–µ—Ç–∞:
1. **–ö—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ** (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
2. **–û–±—Ä–∞–∑–µ—Ü –¥–∞–Ω–Ω—ã—Ö** (–ø–µ—Ä–≤—ã–µ 10-15 —Å—Ç—Ä–æ–∫ –≤ —Ç–∞–±–ª–∏—Ü–µ)
3. **–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö** (–æ—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏)
4. **–ê–Ω–æ–º–∞–ª–∏–∏** (–∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –µ—Å–ª–∏ –µ—Å—Ç—å, –∏–ª–∏ "–Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
5. **–ò—Ç–æ–≥** (1 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ)

–ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º, –∏—Å–ø–æ–ª—å–∑—É–π —á–∏—Å–ª–∞, –ø—Ä–∏–º–µ—Ä—ã. –ù–ï –ø–æ–∫–∞–∑—ã–≤–∞–π –ø—Ä–æ—Ü–µ—Å—Å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π.
"""
    
    prompt = f"""
–ò—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å: "{analysis_context['original_query']}"
–¢–∏–ø –∞–Ω–∞–ª–∏–∑–∞: {analysis_context['analysis_type']}
–ó–∞–ø–∏—Å–µ–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {analysis_context['total_records']}
–î–∞–Ω–Ω—ã–µ –¥–æ—Å—Ç—É–ø–Ω—ã: {'–î–∞' if analysis_context['data_available'] else '–ù–µ—Ç'}

"""
    
    if analysis_context['data_available']:
        if 'key_statistics' in analysis_context:
            stats = analysis_context['key_statistics']
            prompt += f"""
–û—Å–Ω–æ–≤–Ω–æ–π –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å: {stats['column']}
–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: —Å—Ä–µ–¥–Ω–µ–µ={stats['mean']}, –º–∏–Ω={stats['min']}, –º–∞–∫—Å={stats['max']}, œÉ={stats['std']}

–ü–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–æ–∫ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–∫–∞–∑–∞:
{analysis_context.get('sample_data', [])}

"""
    
    if 'anomaly_analysis' in analysis_context:
        anomaly_data = analysis_context['anomaly_analysis']
        if anomaly_data.get('found', False):
            prompt += f"""
–ê–ù–û–ú–ê–õ–ò–ò –ù–ê–ô–î–ï–ù–´:
–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ: {anomaly_data.get('summary', '–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∞–Ω–æ–º–∞–ª–∏–∏')}

–î–µ—Ç–∞–ª–∏ (—Ç–æ–ø-10):
"""
            for detail in anomaly_data.get('details', [])[:10]:
                prompt += f"- {detail['name']}: {detail['count']} –∞–Ω–æ–º–∞–ª–∏–π ({detail['percentage']}%)"
                if 'examples' in detail:
                    prompt += f", –ø—Ä–∏–º–µ—Ä—ã: {detail['examples']}"
                prompt += "\n"
        else:
            prompt += f"–ê–ù–û–ú–ê–õ–ò–ò –ù–ï –ù–ê–ô–î–ï–ù–´: {anomaly_data.get('message', '–í—Å–µ –¥–∞–Ω–Ω—ã–µ –≤ –Ω–æ—Ä–º–µ')}\n"
    
    prompt += "\n–°–æ–∑–¥–∞–π –∫–æ–º–ø–∞–∫—Ç–Ω–æ–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–∞–º–º–∞—Ä–∏:"
    
    try:
        llm_response = call_giga_api_wrapper(prompt, system_instruction)
        return format_compact_llm_summary(llm_response, analysis_context)
    except Exception as e:
        print(f"[COMPACT_LLM_SUMMARY ERROR] –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
        return generate_compact_fallback_from_context(analysis_context)

def format_compact_llm_summary(llm_response, analysis_context):
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∫–æ–º–ø–∞–∫—Ç–Ω–æ–µ LLM —Å–∞–º–º–∞—Ä–∏"""
    analysis_type_name = "üîç –ê–Ω–∞–ª–∏–∑ –∞–Ω–æ–º–∞–ª–∏–π" if analysis_context['analysis_type'] == 'anomaly_detection' else "üìä –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö"
    
    formatted_summary = f"# {analysis_type_name}\n\n"
    formatted_summary += f"**–ó–∞–ø—Ä–æ—Å:** {analysis_context['original_query']}\n"
    formatted_summary += f"**–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ:** {analysis_context['total_records']:,} –∑–∞–ø–∏—Å–µ–π\n\n"
    
    # LLM –∞–Ω–∞–ª–∏–∑
    formatted_summary += llm_response + "\n\n"
    
    # –ü–ï–†–í–´–ï 10-15 –°–¢–†–û–ö –î–ê–ù–ù–´–• (–µ—Å–ª–∏ –µ—Å—Ç—å)
    if analysis_context['data_available'] and 'sample_data' in analysis_context:
        formatted_summary += "**üìã –û–±—Ä–∞–∑–µ—Ü –¥–∞–Ω–Ω—ã—Ö (–ø–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–æ–∫):**\n\n"
        sample_df = pd.DataFrame(analysis_context['sample_data'])
        formatted_summary += sample_df.to_markdown(index=False) + "\n\n"
    
    # –ö–û–ú–ü–ê–ö–¢–ù–´–ï –ê–ù–û–ú–ê–õ–ò–ò
    if 'anomaly_analysis' in analysis_context and analysis_context['anomaly_analysis'].get('found', False):
        anomaly_data = analysis_context['anomaly_analysis']
        formatted_summary += "**üéØ –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∞–Ω–æ–º–∞–ª–∏–∏ (—Ç–æ–ø-10):**\n"
        
        for i, detail in enumerate(anomaly_data.get('details', [])[:10], 1):
            name = detail.get('name', f'–û–±—ä–µ–∫—Ç {i}')
            count = detail.get('count', 0)
            formatted_summary += f"{i}. **{name}**: {count} –∞–Ω–æ–º–∞–ª–∏–π"
            
            if 'examples' in detail:
                examples_str = ", ".join([str(x) for x in detail['examples'][:3]])
                formatted_summary += f" (–ø—Ä–∏–º–µ—Ä—ã: {examples_str})"
            formatted_summary += "\n"
        
        formatted_summary += "\n"
    
    # –ö–†–ê–¢–ö–ò–ô –ò–¢–û–ì
    formatted_summary += f"**üìä –ò—Ç–æ–≥:** {analysis_context['total_records']:,} –∑–∞–ø–∏—Å–µ–π"
    if 'anomaly_analysis' in analysis_context and analysis_context['anomaly_analysis'].get('found', False):
        total_anomalies = sum(d.get('count', 0) for d in analysis_context['anomaly_analysis'].get('details', []))
        formatted_summary += f" ‚Ä¢ {total_anomalies} –∞–Ω–æ–º–∞–ª–∏–π –Ω–∞–π–¥–µ–Ω–æ"
    else:
        formatted_summary += " ‚Ä¢ –∞–Ω–æ–º–∞–ª–∏–∏ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã"
    
    return formatted_summary

def generate_compact_fallback_from_context(analysis_context):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ–º–ø–∞–∫—Ç–Ω–æ–µ —Ä–µ–∑–µ—Ä–≤–Ω–æ–µ —Å–∞–º–º–∞—Ä–∏"""
    analysis_type_name = "üîç –ê–Ω–∞–ª–∏–∑ –∞–Ω–æ–º–∞–ª–∏–π" if analysis_context['analysis_type'] == 'anomaly_detection' else "üìä –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö"
    
    summary = f"# {analysis_type_name}\n\n"
    summary += f"**–ó–∞–ø—Ä–æ—Å:** {analysis_context['original_query']}\n"
    summary += f"**–†–µ–∑—É–ª—å—Ç–∞—Ç:** –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {analysis_context['total_records']} –∑–∞–ø–∏—Å–µ–π\n\n"
    
    if analysis_context['data_available']:
        if 'key_statistics' in analysis_context:
            stats = analysis_context['key_statistics']
            summary += f"**–û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏:**\n"
            summary += f"‚Ä¢ –°—Ç–æ–ª–±–µ—Ü: {stats['column']}\n"
            summary += f"‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ: {stats['mean']}\n"
            summary += f"‚Ä¢ –î–∏–∞–ø–∞–∑–æ–Ω: {stats['min']} ‚Äî {stats['max']}\n\n"
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –æ–±—Ä–∞–∑–µ—Ü –¥–∞–Ω–Ω—ã—Ö
        if 'sample_data' in analysis_context:
            summary += "**üìã –û–±—Ä–∞–∑–µ—Ü –¥–∞–Ω–Ω—ã—Ö:**\n\n"
            sample_df = pd.DataFrame(analysis_context['sample_data'])
            summary += sample_df.head(10).to_markdown(index=False) + "\n\n"
    else:
        summary += "**‚ö†Ô∏è –î–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã**\n\n"
    
    if 'anomaly_analysis' in analysis_context:
        if analysis_context['anomaly_analysis'].get('found', False):
            summary += "**üö® –ù–∞–π–¥–µ–Ω—ã –∞–Ω–æ–º–∞–ª–∏–∏** - –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–∞–Ω–Ω—ã–µ\n"
        else:
            summary += "**‚úÖ –ê–Ω–æ–º–∞–ª–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã** - –¥–∞–Ω–Ω—ã–µ –≤ –Ω–æ—Ä–º–µ\n"
    
    return summary

def generate_compact_fallback_summary(sql_results_df, user_prompt, anomaly_results, needs_anomaly_detection):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Ä–æ—Å—Ç–æ–µ –∫–æ–º–ø–∞–∫—Ç–Ω–æ–µ —Ä–µ–∑–µ—Ä–≤–Ω–æ–µ —Å–∞–º–º–∞—Ä–∏"""
    if sql_results_df is None:
        return "**‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞**\n\n–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –∑–∞–ø—Ä–æ—Å–∞."
    
    if sql_results_df.empty:
        return f"**üì≠ –î–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã**\n\n–ü–æ –∑–∞–ø—Ä–æ—Å—É '{user_prompt}' –¥–∞–Ω–Ω—ã–µ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã."
    
    summary = f"**üìä –ö—Ä–∞—Ç–∫–∏–π –∞–Ω–∞–ª–∏–∑**\n\n"
    summary += f"**–ó–∞–ø—Ä–æ—Å:** {user_prompt}\n"
    summary += f"**–ó–∞–ø–∏—Å–µ–π:** {len(sql_results_df):,}\n\n"
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–æ–∫
    summary += "**üìã –î–∞–Ω–Ω—ã–µ (–ø–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–æ–∫):**\n\n"
    summary += sql_results_df.head(10).to_markdown(index=False) + "\n\n"
    
    if needs_anomaly_detection and anomaly_results:
        if anomaly_results.get('anomalies_found', False):
            summary += "**üö® –ê–Ω–æ–º–∞–ª–∏–∏ –Ω–∞–π–¥–µ–Ω—ã**\n"
        else:
            summary += "**‚úÖ –ê–Ω–æ–º–∞–ª–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã**\n"
    
    return summary

# ==============================================================================
# –û–°–ù–û–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø-–ì–ï–ù–ï–†–ê–¢–û–† (–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø)
# ==============================================================================

async def event_generator(user_prompt: str, duckdb_con):
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å–æ–±—ã—Ç–∏–π"""
    current_stage = "–ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏"
    current_data = {}
    rag_context_for_agents = None
    
    # Accumulated data for potential saving
    accumulated_data_for_rag = {
        "user_prompt": user_prompt,
        "formal_prompt": None,
        "plan": None,
        "sql_query": None,
        "final_answer": None
    }

    try:
        yield {"type": "status", "stage": "rag_check", "message": "–ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π..."}
        rag_result = agent_rag_retriever(user_prompt)
        rag_status = rag_result.get("status")
        rag_data = rag_result.get("data")

        if rag_status == "error":
            yield {"type": "error", "stage": "rag_check", "message": f"–û—à–∏–±–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π: {rag_result.get('message')}. –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ –Ω–µ–µ."}
        elif rag_status == "exact":
            final_answer = rag_data.get("final_answer", "–¢–æ—á–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞–π–¥–µ–Ω.")
            accumulated_data_for_rag["formal_prompt"] = rag_data.get("formal_prompt")
            accumulated_data_for_rag["plan"] = rag_data.get("plan")
            accumulated_data_for_rag["sql_query"] = rag_data.get("sql_query")
            accumulated_data_for_rag["final_answer"] = final_answer
            yield {"type": "final_answer", "stage": "rag_exact_match", "answer": final_answer, "rag_components": accumulated_data_for_rag, "message": "–ù–∞–π–¥–µ–Ω —Ç–æ—á–Ω—ã–π –æ—Ç–≤–µ—Ç."}
            return
        elif rag_status == "similar":
            rag_context_for_agents = rag_data
            yield {"type": "status", "stage": "rag_similar_match", "message": "–ù–∞–π–¥–µ–Ω –ø–æ—Ö–æ–∂–∏–π –∑–∞–ø—Ä–æ—Å. –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è."}
        else:
            yield {"type": "status", "stage": "rag_no_match", "message": "–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞."}

        # –ê–Ω–∞–ª–∏–∑ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏
        current_stage = "–ê–Ω–∞–ª–∏–∑ –∑–∞–ø—Ä–æ—Å–∞"
        yield {"type": "status", "stage": "coordinator", "message": current_stage + "..."}
        ambiguity_analysis_result, enhanced_prompt, needs_clarification, needs_anomaly_detection_local = await analyze_query_ambiguity(user_prompt)
        
        if needs_clarification:
            yield {"type": "clarification_needed", "stage": "ambiguity_analysis", "data": ambiguity_analysis_result}
            return

        if enhanced_prompt and enhanced_prompt != user_prompt:
            yield {"type": "status", "stage": "ambiguity_analysis", "message": f"–ó–∞–ø—Ä–æ—Å —É–ª—É—á—à–µ–Ω: {enhanced_prompt}", "original_prompt": user_prompt, "enhanced_prompt": enhanced_prompt}
            processed_user_prompt = enhanced_prompt
        else:
            processed_user_prompt = user_prompt
        
        needs_anomaly_detection = needs_anomaly_detection_local

        # 1. –ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤—â–∏–∫
        current_stage = "–ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞"
        yield {"type": "status", "stage": "rephraser", "message": current_stage + "..."}
        formal_prompt_result = agent1_rephraser(processed_user_prompt, rag_context=rag_context_for_agents)
        accumulated_data_for_rag["formal_prompt"] = formal_prompt_result
        yield {"type": "intermediary_result", "stage": "rephraser", "name": "–§–æ—Ä–º–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å", "content": formal_prompt_result}

        # 2. –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫
        current_stage = "–°–æ–∑–¥–∞–Ω–∏–µ –ø–ª–∞–Ω–∞"
        yield {"type": "status", "stage": "planner", "message": current_stage + "..."}
        plan_result = agent2_planner(formal_prompt_result, rag_context=rag_context_for_agents)
        accumulated_data_for_rag["plan"] = plan_result
        yield {"type": "intermediary_result", "stage": "planner", "name": "–ü–ª–∞–Ω –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è", "content": plan_result}

        # 3. SQL –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä
        current_stage = "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è SQL"
        yield {"type": "status", "stage": "sql_generator", "message": current_stage + "..."}
        sql_query_result = agent3_sql_generator(plan_result, rag_context=rag_context_for_agents)
        yield {"type": "intermediary_result", "stage": "sql_generator", "name": "SQL", "content": sql_query_result, "language": "sql"}

        # 4. –í–∞–ª–∏–¥–∞—Ü–∏—è SQL (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è)
        current_stage = "–í–∞–ª–∏–¥–∞—Ü–∏—è SQL"
        yield {"type": "status", "stage": "sql_validation", "message": current_stage + "..."}
        validator_result = agent_sql_validator(sql_query_result, plan_result)

        if validator_result.get("is_valid"):
            validated_sql = validator_result.get("corrected_sql") or sql_query_result
            yield {"type": "intermediary_result", "stage": "sql_validation", "name": "–í–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π SQL", "content": validated_sql, "language": "sql"}
        else:
            # –ü—Ä–æ—Å—Ç–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
            current_stage = "–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ SQL"
            yield {"type": "status", "stage": "sql_fixer", "message": current_stage + "..."}
            fixed_sql = agent_sql_fixer(sql_query_result, validator_result.get("message", ""), plan_result)
            validated_sql = fixed_sql if fixed_sql and fixed_sql.strip() else sql_query_result
            yield {"type": "intermediary_result", "stage": "sql_fixer", "name": "–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π SQL", "content": validated_sql, "language": "sql"}
        
        accumulated_data_for_rag["sql_query"] = validated_sql

        # 5. –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ SQL
        current_stage = "–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ SQL"
        yield {"type": "status", "stage": "sql_execution", "message": current_stage + "..."}
        try:
            sql_results_df = await execute_duckdb_query(duckdb_con, validated_sql)
            if sql_results_df is not None and not sql_results_df.empty:
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º datetime –≤ —Å—Ç—Ä–æ–∫–∏
                for col in sql_results_df.columns:
                    if pd.api.types.is_datetime64_any_dtype(sql_results_df[col]):
                        sql_results_df[col] = sql_results_df[col].astype(str)
                
                # –û–ì–†–ê–ù–ò–ß–ò–í–ê–ï–ú –í–û–ó–í–†–ê–©–ê–ï–ú–´–ï –î–ê–ù–ù–´–ï –î–û 100 –°–¢–†–û–ö
                display_df = sql_results_df.head(100)
                sql_results_json = display_df.to_dict(orient='records')
                
                yield {"type": "sql_result", "stage": "sql_execution", "data": sql_results_json, "row_count": len(sql_results_df), "message": f"SQL –≤—ã–ø–æ–ª–Ω–µ–Ω. –ü–æ–∫–∞–∑–∞–Ω–æ {len(display_df)} –∏–∑ {len(sql_results_df)} —Å—Ç—Ä–æ–∫."}
            else:
                sql_results_json = []
                yield {"type": "sql_result", "stage": "sql_execution", "data": [], "row_count": 0, "message": "SQL –≤—ã–ø–æ–ª–Ω–µ–Ω, –Ω–æ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç."}
        except Exception as e:
            yield {"type": "error", "stage": "sql_execution", "message": f"–û—à–∏–±–∫–∞ SQL: {str(e)}"}
            return

        # 6. –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –∞–Ω–∞–ª–∏–∑ –∞–Ω–æ–º–∞–ª–∏–π (—Ç–æ–ø-10)
        anomaly_summary = None
        if needs_anomaly_detection:
            current_stage = "–ê–Ω–∞–ª–∏–∑ –∞–Ω–æ–º–∞–ª–∏–π (—Ç–æ–ø-10)"
            yield {"type": "status", "stage": "anomaly_detection", "message": current_stage + "..."}
            if sql_results_df is not None and not sql_results_df.empty:
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∞–Ω–∞–ª–∏–∑ –ø–µ—Ä–≤—ã–º–∏ 1000 —Å—Ç—Ä–æ–∫–∞–º–∏ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                analysis_df = sql_results_df.head(1000)
                anomaly_results = agent_anomaly_detector(analysis_df, processed_user_prompt)
                yield {"type": "intermediary_result", "stage": "anomaly_detection", "name": "–ê–Ω–æ–º–∞–ª–∏–∏ (—Ç–æ–ø-10)", "content": anomaly_results}
                anomaly_summary = anomaly_results
            else:
                yield {"type": "warning", "stage": "anomaly_detection", "message": "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∞–Ω–æ–º–∞–ª–∏–π."}

        # 7. –ö–û–ú–ü–ê–ö–¢–ù–ê–Ø –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
        current_stage = "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–º–ø–∞–∫—Ç–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞"
        yield {"type": "status", "stage": "final_summary_generation", "message": current_stage + "..."}
        llm_summary_result = generate_llm_analysis_summary(
            sql_results_df, 
            processed_user_prompt, 
            anomaly_results=anomaly_summary,
            needs_anomaly_detection=needs_anomaly_detection,
            rag_context=rag_context_for_agents
        )
        accumulated_data_for_rag["final_answer"] = llm_summary_result
        yield {"type": "final_answer", "stage": "final_summary_generation", "answer": llm_summary_result, "rag_components": accumulated_data_for_rag, "message": "–ö–æ–º–ø–∞–∫—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç –≥–æ—Ç–æ–≤."}

    except Exception as e:
        error_message = f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –Ω–∞ —ç—Ç–∞–ø–µ '{current_stage}': {str(e)}"
        print(f"[EVENT_GENERATOR ERROR] {error_message}")
        yield {"type": "error", "stage": current_stage, "message": error_message}
    finally:
        yield {"type": "status", "stage": "done", "message": "–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞."}

# ==============================================================================
# –≠–ù–î–ü–û–ò–ù–¢–´ (–ë–ï–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô)
# ==============================================================================

@app.get("/process_query_stream")
async def process_query_stream(user_prompt: str, request: Request):
    """Endpoint –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –ø–æ—Ç–æ–∫–æ–≤—ã–º –æ—Ç–≤–µ—Ç–æ–º"""
    duckdb_con = request.app.state.duckdb_con
    if not duckdb_con:
        return JSONResponse(status_code=500, content={"error": "–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞."})
    
    if not user_prompt or not user_prompt.strip():
        return JSONResponse(status_code=400, content={"error": "user_prompt –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω –∏–ª–∏ –ø—É—Å—Ç."})

    print(f"\n[API] –ü–æ–ª—É—á–µ–Ω –∑–∞–ø—Ä–æ—Å: {user_prompt}")

    async def format_sse(data: dict) -> str:
        """Format data as SSE message"""
        return f"data: {json.dumps(data)}\n\n"

    async def event_stream():
        async for event in event_generator(user_prompt.strip(), duckdb_con):
            yield await format_sse(event)

    return StreamingResponse(
        event_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "*",
            "Access-Control-Allow-Methods": "*"
        }
    )

@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"""
    try:
        if hasattr(app.state, 'duckdb_con') and app.state.duckdb_con:
            return {"status": "healthy", "service": "text2sql-optimized", "database": "connected"}
        else:
            return {"status": "unhealthy", "service": "text2sql-optimized", "database": "disconnected"}
    except Exception as e:
        return {"status": "unhealthy", "service": "text2sql-optimized", "error": str(e)}

@app.get("/tables")
async def get_available_tables():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü"""
    return {
        "tables": [
            {"name": "population", "description": "–î–∞–Ω–Ω—ã–µ –æ —á–∏—Å–ª–µ–Ω–Ω–æ—Å—Ç–∏ –Ω–∞—Å–µ–ª–µ–Ω–∏—è –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º", "type": "demographic"},
            {"name": "salary", "description": "–î–∞–Ω–Ω—ã–µ –æ –∑–∞—Ä–∞–±–æ—Ç–Ω—ã—Ö –ø–ª–∞—Ç–∞—Ö –ø–æ –æ—Ç—Ä–∞—Å–ª—è–º", "type": "economic"},
            {"name": "migration", "description": "–î–∞–Ω–Ω—ã–µ –æ –º–∏–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –ø–æ—Ç–æ–∫–∞—Ö", "type": "demographic"},
            {"name": "market_access", "description": "–ò–Ω–¥–µ–∫—Å—ã –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —Ä—ã–Ω–∫–æ–≤", "type": "economic"},
            {"name": "connections", "description": "–î–∞–Ω–Ω—ã–µ –æ —Å–≤—è–∑—è—Ö –º–µ–∂–¥—É —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏—è–º–∏", "type": "infrastructure"}
        ]
    }

# ==============================================================================
# EXCEPTION HANDLERS –ò STARTUP/SHUTDOWN
# ==============================================================================

@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    """–ì–ª–æ–±–∞–ª—å–Ω—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –∏—Å–∫–ª—é—á–µ–Ω–∏–π"""
    print(f"[GLOBAL ERROR] –ù–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {exc}")
    return JSONResponse(
        status_code=500,
        content={
            "error": "–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞",
            "message": "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.",
            "type": "internal_server_error"
        }
    )

@app.on_event("startup")
async def startup_event():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ"""
    print("üöÄ –ó–∞–ø—É—Å–∫ Optimized Text2SQL Multi-Agent System...")
    
    try:
        duckdb_connection = setup_duckdb()
        if duckdb_connection is not None:
            app.state.duckdb_con = duckdb_connection
            print("‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö DuckDB –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
        else:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö")
            app.state.duckdb_con = None

        # Initialize RAG database
        print("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π...")
        if not init_rag_db():
            print("[STARTUP ERROR] –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å RAG –±–∞–∑—É.")
        else:
            print("‚úÖ RAG –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
        app.state.duckdb_con = None

@app.on_event("shutdown")
async def shutdown_event():
    """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤ –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏"""
    print("üõë –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã Optimized Text2SQL System...")
    
    try:
        if (hasattr(app.state, 'duckdb_con') and 
            app.state.duckdb_con is not None and
            hasattr(app.state.duckdb_con, 'close')):
            app.state.duckdb_con.close()
            print("‚úÖ –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö –∑–∞–∫—Ä—ã—Ç–æ")
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–∫—Ä—ã—Ç–∏–∏ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è: {e}")

# ==============================================================================
# RAG SAVE ENDPOINT
# ==============================================================================

class RagSaveRequest(BaseModel):
    user_prompt: str
    formal_prompt: str
    plan: str
    sql_query: str
    final_answer: str

@app.post("/llm_query/confirm_and_save")
async def confirm_answer_and_save_to_rag(payload: RagSaveRequest):
    """Endpoint –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –≤ RAG –±–∞–∑—É"""
    print(f"[API /confirm_and_save] –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ RAG: {payload.user_prompt[:50]}...")
    try:
        loop = asyncio.get_event_loop()
        success = await loop.run_in_executor(None, lambda: add_to_rag_db(
            user_prompt=payload.user_prompt,
            formal_prompt=payload.formal_prompt,
            plan=payload.plan,
            sql_query=payload.sql_query,
            final_answer=payload.final_answer
        ))

        if success:
            print(f"[API /confirm_and_save] –£—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {payload.user_prompt[:50]}")
            return JSONResponse(content={"status": "success", "message": "–û—Ç–≤–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π."}, status_code=200)
        else:
            print(f"[API /confirm_and_save] –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {payload.user_prompt[:50]}")
            raise HTTPException(status_code=500, detail="–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π.")
    except Exception as e:
        print(f"[API /confirm_and_save] –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ RAG: {str(e)}")

# ==============================================================================
# –ó–ê–ü–£–°–ö –°–ï–†–í–ï–†–ê
# ==============================================================================

if __name__ == "__main__":
    print("üöÄ –ó–∞–ø—É—Å–∫ Optimized Text2SQL Multi-Agent System...")
    
    if not os.path.isdir(STATIC_DIR):
        print(f"[CRITICAL ERROR] Static –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {STATIC_DIR}")
    elif not os.path.exists(INDEX_HTML_PATH):
        print(f"[CRITICAL ERROR] index.html –Ω–µ –Ω–∞–π–¥–µ–Ω: {INDEX_HTML_PATH}")
    
    import uvicorn
    uvicorn.run(
        app, 
        host="0.0.0.0", 
        port=5002,
        reload=False,
        log_level="info",
        access_log=True
    )