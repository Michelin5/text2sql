import asyncio
import json
import numpy as np
from fastapi import FastAPI, Request, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse, FileResponse
import pandas as pd
from pydantic import BaseModel
from json.decoder import JSONDecodeError
from fastapi.staticfiles import StaticFiles
import os

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∞–≥–µ–Ω—Ç–æ–≤ –∏ —É—Ç–∏–ª–∏—Ç—ã
from giga_wrapper import giga_client, call_giga_api_wrapper
from agents import (
    agent0_coordinator,
    agent1_rephraser,
    agent2_planner,
    agent3_sql_generator,
    agent_sql_validator,
    agent_sql_fixer,
    agent_anomaly_detector,
    agent_rag_retriever
)
from duckdb_utils import setup_duckdb, execute_duckdb_query
from rag_handler import init_rag_db, add_to_rag_db

# Determine the absolute path to the project directory
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
STATIC_DIR = os.path.join(BASE_DIR, "static")
INDEX_HTML_PATH = os.path.join(STATIC_DIR, "index.html")

app = FastAPI()

# CORS setup
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Mount static files - Use the absolute STATIC_DIR path
app.mount("/static", StaticFiles(directory=STATIC_DIR), name="static")

# Root endpoint to serve index.html - THIS SHOULD BE THE ONLY @app.get("/")
@app.get("/")
async def read_index():
    # Use the absolute INDEX_HTML_PATH
    # Also, check if the file exists before trying to serve it for better error handling
    if not os.path.exists(INDEX_HTML_PATH):
        print(f"[ERROR] index.html not found at: {INDEX_HTML_PATH}")
        raise HTTPException(status_code=404, detail=f"index.html not found at {INDEX_HTML_PATH}")
    print(f"[INFO] Serving index.html from: {INDEX_HTML_PATH}")
    return FileResponse(INDEX_HTML_PATH)

# ==============================================================================
# –û–ë–†–ê–ë–û–¢–ö–ê –ù–ï–û–ü–†–ï–î–ï–õ–ï–ù–ù–û–°–¢–ò –ò –£–¢–û–ß–ù–ï–ù–ò–ô
# ==============================================================================

async def analyze_query_ambiguity(user_prompt):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å –∑–∞–ø—Ä–æ—Å–∞ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä–∞"""
    prompt_lower = user_prompt.lower()
    
    # –î–µ—Ç–µ–∫—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π
    anomaly_keywords = ['–∞–Ω–æ–º–∞–ª–∏', '–≤—ã–±—Ä–æ—Å', '–Ω–µ–æ–±—ã—á–Ω', '—Å—Ç—Ä–∞–Ω–Ω', '–æ—Ç–∫–ª–æ–Ω–µ–Ω–∏', '—ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω', '–ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å']
    needs_anomaly = any(keyword in prompt_lower for keyword in anomaly_keywords)
    
    # –î–µ—Ç–µ–∫—Ü–∏—è —Ç–∞–±–ª–∏—Ü –≤ –∑–∞–ø—Ä–æ—Å–µ
    table_mentions = {
        'population': ['–Ω–∞—Å–µ–ª–µ–Ω', '–¥–µ–º–æ–≥—Ä–∞—Ñ', '–ª—é–¥–µ–π', '–∂–∏—Ç–µ–ª'],
        'salary': ['–∑–∞—Ä–ø–ª–∞—Ç', '–∑–∞—Ä–∞–±–æ—Ç–Ω', '–æ–∫–ª–∞–¥', '–¥–æ—Ö–æ–¥'],
        'migration': ['–º–∏–≥—Ä–∞', '–ø–µ—Ä–µ–µ–∑–¥', '–ø–µ—Ä–µ–º–µ—â–µ–Ω'],
        'market_access': ['—Ä—ã–Ω–æ–∫', '–¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç', '—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫'],
        'connections': ['—Å–≤—è–∑', '—Å–æ–µ–¥–∏–Ω–µ–Ω', '–º–∞—Ä—à—Ä—É—Ç']
    }
    
    mentioned_tables = []
    for table, keywords in table_mentions.items():
        if any(keyword in prompt_lower for keyword in keywords):
            mentioned_tables.append(table)
    
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Ä–æ–≤–Ω—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏
    ambiguity_level = "low"
    needs_clarification = False
    enhanced_prompt = user_prompt
    
    # –í—ã—Å–æ–∫–∞—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å: –∞–Ω–æ–º–∞–ª–∏–∏ –±–µ–∑ —É–∫–∞–∑–∞–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã
    if (needs_anomaly and not mentioned_tables and len(user_prompt.split()) < 8):
        ambiguity_level = "high"
        needs_clarification = True
        
        clarification_data = {
            "needs_clarification": True,
            "ambiguity_level": ambiguity_level,
            "reason": "–ó–∞–ø—Ä–æ—Å –æ–± –∞–Ω–æ–º–∞–ª–∏—è—Ö –±–µ–∑ —É–∫–∞–∑–∞–Ω–∏—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã",
            "message": "–î–ª—è –ø–æ–∏—Å–∫–∞ –∞–Ω–æ–º–∞–ª–∏–π —É–∫–∞–∂–∏—Ç–µ, –≤ –∫–∞–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏—Å–∫–∞—Ç—å:",
            "suggested_tables": [
                {"id": "population", "name": "–ù–∞—Å–µ–ª–µ–Ω–∏–µ", "description": "–î–∞–Ω–Ω—ã–µ –æ —á–∏—Å–ª–µ–Ω–Ω–æ—Å—Ç–∏ –Ω–∞—Å–µ–ª–µ–Ω–∏—è –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º"},
                {"id": "salary", "name": "–ó–∞—Ä–ø–ª–∞—Ç—ã", "description": "–î–∞–Ω–Ω—ã–µ –æ –∑–∞—Ä–∞–±–æ—Ç–Ω—ã—Ö –ø–ª–∞—Ç–∞—Ö –ø–æ –æ—Ç—Ä–∞—Å–ª—è–º"},
                {"id": "migration", "name": "–ú–∏–≥—Ä–∞—Ü–∏—è", "description": "–î–∞–Ω–Ω—ã–µ –æ –º–∏–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –ø–æ—Ç–æ–∫–∞—Ö"},
                {"id": "market_access", "name": "–î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —Ä—ã–Ω–∫–æ–≤", "description": "–ò–Ω–¥–µ–∫—Å—ã –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —Ä—ã–Ω–∫–æ–≤"},
                {"id": "connections", "name": "–°–≤—è–∑–∏", "description": "–î–∞–Ω–Ω—ã–µ –æ —Å–≤—è–∑—è—Ö –º–µ–∂–¥—É —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏—è–º–∏"}
            ],
            "examples": [
                "–ù–∞–π–¥–∏ –∞–Ω–æ–º–∞–ª–∏–∏ –≤ –¥–∞–Ω–Ω—ã—Ö –æ –Ω–∞—Å–µ–ª–µ–Ω–∏–∏",
                "–ü–æ–∫–∞–∂–∏ –≤—ã–±—Ä–æ—Å—ã –≤ –∑–∞—Ä–ø–ª–∞—Ç–∞—Ö –ø–æ –æ—Ç—Ä–∞—Å–ª—è–º",
                "–û–±–Ω–∞—Ä—É–∂—å –Ω–µ–æ–±—ã—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –º–∏–≥—Ä–∞—Ü–∏–∏",
                "–ù–∞–π–¥–∏ –∞–Ω–æ–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —Ä—ã–Ω–∫–æ–≤"
            ]
        }
        return clarification_data, None, needs_clarification, needs_anomaly
    
    # –°—Ä–µ–¥–Ω—è—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ
    elif (needs_anomaly and not mentioned_tables):
        ambiguity_level = "medium"
        enhanced_prompt = f"–ù–∞–π–¥–∏ –∞–Ω–æ–º–∞–ª–∏–∏ –≤ –¥–∞–Ω–Ω—ã—Ö –æ –º–∏–≥—Ä–∞—Ü–∏–∏"
        
        enhancement_data = {
            "ambiguity_level": ambiguity_level,
            "original_prompt": user_prompt,
            "enhanced_prompt": enhanced_prompt,
            "reason": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±—Ä–∞–Ω–∞ —Ç–∞–±–ª–∏—Ü–∞ '–º–∏–≥—Ä–∞—Ü–∏—è' –¥–ª—è –ø–æ–∏—Å–∫–∞ –∞–Ω–æ–º–∞–ª–∏–π",
            "auto_selected_table": "migration"
        }
        return enhancement_data, enhanced_prompt, False, needs_anomaly
    
    # –ù–∏–∑–∫–∞—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å
    return None, enhanced_prompt, False, needs_anomaly

# ==============================================================================
# LLM-–ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –ò –ì–ï–ù–ï–†–ê–¶–ò–Ø –°–ê–ú–ú–ê–†–ò
# ==============================================================================

def generate_llm_analysis_summary(sql_results_df, user_prompt, anomaly_results=None, needs_anomaly_detection=False, rag_context: dict = None):
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç LLM-–∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —Å–∞–º–º–∞—Ä–∏
    """
    try:
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        analysis_context = prepare_analysis_context(sql_results_df, user_prompt, anomaly_results, needs_anomaly_detection)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º LLM-–∞–Ω–∞–ª–∏–∑
        llm_summary = generate_intelligent_summary(analysis_context, rag_context=rag_context)
        
        return llm_summary
    except Exception as e:
        print(f"[LLM_ANALYSIS ERROR] –û—à–∏–±–∫–∞ –≤ LLM-–∞–Ω–∞–ª–∏–∑–µ: {e}")
        return generate_fallback_summary(sql_results_df, user_prompt, anomaly_results, needs_anomaly_detection)

def prepare_analysis_context(sql_results_df, user_prompt, anomaly_results, needs_anomaly_detection):
    """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è LLM-–∞–Ω–∞–ª–∏–∑–∞"""
    context = {
        "original_query": user_prompt,
        "data_available": sql_results_df is not None and not sql_results_df.empty,
        "total_records": len(sql_results_df) if sql_results_df is not None else 0,
        "analysis_type": "anomaly_detection" if needs_anomaly_detection else "standard_analysis"
    }
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞–Ω–Ω—ã—Ö
    if sql_results_df is not None and not sql_results_df.empty:
        context["columns"] = list(sql_results_df.columns)
        context["data_types"] = {col: str(dtype) for col, dtype in sql_results_df.dtypes.items()}
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —á–∏—Å–ª–æ–≤—ã–º —Å—Ç–æ–ª–±—Ü–∞–º
        numeric_cols = sql_results_df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            context["statistics"] = {}
            for col in numeric_cols:
                col_data = sql_results_df[col].dropna()
                if len(col_data) > 0:
                    context["statistics"][col] = {
                        "mean": float(col_data.mean()),
                        "median": float(col_data.median()),
                        "std": float(col_data.std()),
                        "min": float(col_data.min()),
                        "max": float(col_data.max())
                    }
        
        # –û–±—Ä–∞–∑–µ—Ü –¥–∞–Ω–Ω—ã—Ö
        context["sample_data"] = sql_results_df.head(5).to_dict('records')
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –∞–Ω–æ–º–∞–ª–∏–π
    if anomaly_results:
        context["anomaly_analysis"] = anomaly_results
    
    return context

def generate_intelligent_summary(analysis_context, rag_context: dict = None):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ —Å–∞–º–º–∞—Ä–∏ —Å –ø–æ–º–æ—â—å—é LLM"""
    rag_guidance = ""
    if rag_context and isinstance(rag_context, dict) and rag_context.get('final_answer'):
        rag_guidance = (
            "–¢–µ–±–µ —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–µ–≥–æ —É—Å–ø–µ—à–Ω–æ–≥–æ –ø—Ä–æ—à–ª–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞, –≤–∫–ª—é—á–∞—è –µ–≥–æ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç. "
            "–ò—Å–ø–æ–ª—å–∑—É–π —Å—Ç–∏–ª—å, —Ç–æ–Ω –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–æ—à–ª–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ (`–ö–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–æ—à–ª–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ (—Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç)`) –∫–∞–∫ —Ö–æ—Ä–æ—à–∏–π –ø—Ä–∏–º–µ—Ä. "
            "–û–¥–Ω–∞–∫–æ, —É–±–µ–¥–∏—Å—å, —á—Ç–æ –¢–ï–ö–£–©–ò–ô –æ—Ç–≤–µ—Ç –¢–û–ß–ù–û –æ—Ç—Ä–∞–∂–∞–µ—Ç –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –î–ê–ù–ù–´–ï –ò –ê–ù–ê–õ–ò–ó –∏–∑ `–ö–æ–Ω—Ç–µ–∫—Å—Ç –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö`, "
            "–∞ –Ω–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –ø—Ä–æ—à–ª–æ–≥–æ –æ—Ç–≤–µ—Ç–∞. –ê–¥–∞–ø—Ç–∏—Ä—É–π —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –ø–æ–¥ —Ç–µ–∫—É—â–∏–µ –¥–∞–Ω–Ω—ã–µ.\n"
            f"–ö–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–æ—à–ª–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ (—Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç):\n{rag_context.get('final_answer', 'N/A')}\n\n"
        )

    system_instruction = f"""
–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç-–∞–Ω–∞–ª–∏—Ç–∏–∫ –¥–∞–Ω–Ω—ã—Ö. –ù–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞ —Å–æ–∑–¥–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ, –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–µ —Å–∞–º–º–∞—Ä–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.
{rag_guidance}
–¢–≤–æ—è –∑–∞–¥–∞—á–∞:
1. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
2. –í—ã–¥–µ–ª–∏—Ç—å –∫–ª—é—á–µ–≤—ã–µ –∏–Ω—Å–∞–π—Ç—ã –∏ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
3. –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å actionable —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
4. –°—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤—ã–≤–æ–¥—ã –≤ –ø–æ–Ω—è—Ç–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ

–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç–≤–µ—Ç–∞ –¥–æ–ª–∂–Ω–∞ –≤–∫–ª—é—á–∞—Ç—å:
- –ö—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
- –ö–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã (—Å–ø–∏—Å–æ–∫)
- –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –∏–Ω—Å–∞–π—Ç—ã (–µ—Å–ª–∏ –ø—Ä–∏–º–µ–Ω–∏–º–æ)
- –û–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ –∞–Ω–æ–º–∞–ª–∏–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
- –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ (–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã)
- –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∏ –ø—Ä–µ–¥–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω–∏—è (–µ—Å–ª–∏ –µ—Å—Ç—å)

–ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π, –Ω–æ –¥–æ—Å—Ç—É–ø–Ω—ã–π —è–∑—ã–∫. –ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –∏ —Ñ–æ–∫—É—Å–∏—Ä—É–π—Å—è –Ω–∞ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–π —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.
"""
    
    prompt = f"""
–ö–æ–Ω—Ç–µ–∫—Å—Ç –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö:

–ò—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: "{analysis_context['original_query']}"

–¢–∏–ø –∞–Ω–∞–ª–∏–∑–∞: {analysis_context['analysis_type']}
–î–∞–Ω–Ω—ã–µ –¥–æ—Å—Ç—É–ø–Ω—ã: {'–î–∞' if analysis_context['data_available'] else '–ù–µ—Ç'}
–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {analysis_context['total_records']}

"""
    
    if analysis_context['data_available']:
        prompt += f"""
–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö:
- –°—Ç–æ–ª–±—Ü—ã: {', '.join(analysis_context['columns'])}
- –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö: {analysis_context['data_types']}

"""
        
        if 'statistics' in analysis_context:
            prompt += "–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏:\n"
            for col, stats in analysis_context['statistics'].items():
                prompt += f"- {col}: —Å—Ä–µ–¥–Ω–µ–µ={stats['mean']:.2f}, –º–µ–¥–∏–∞–Ω–∞={stats['median']:.2f}, œÉ={stats['std']:.2f}, –º–∏–Ω={stats['min']:.2f}, –º–∞–∫—Å={stats['max']:.2f}\n"
            prompt += "\n"
        
        if 'sample_data' in analysis_context:
            prompt += f"–û–±—Ä–∞–∑–µ—Ü –¥–∞–Ω–Ω—ã—Ö (–ø–µ—Ä–≤—ã–µ 5 –∑–∞–ø–∏—Å–µ–π):\n{analysis_context['sample_data']}\n\n"
    
    if 'anomaly_analysis' in analysis_context:
        anomaly_data = analysis_context['anomaly_analysis']
        prompt += f"""
–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –∞–Ω–æ–º–∞–ª–∏–π:
- –ê–Ω–æ–º–∞–ª–∏–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã: {'–î–∞' if anomaly_data.get('anomalies_found', False) else '–ù–µ—Ç'}
"""
        if anomaly_data.get('anomalies_found', False):
            prompt += f"- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–∏–ø–æ–≤ –∞–Ω–æ–º–∞–ª–∏–π: {anomaly_data.get('anomaly_count', 0)}\n"
            if 'anomalies' in anomaly_data:
                prompt += "–î–µ—Ç–∞–ª–∏ –∞–Ω–æ–º–∞–ª–∏–π:\n"
                for anomaly in anomaly_data['anomalies']:
                    if 'column' in anomaly:
                        prompt += f"  * –°—Ç–æ–ª–±–µ—Ü '{anomaly['column']}': {anomaly['anomaly_count']} –∞–Ω–æ–º–∞–ª–∏–π ({anomaly['anomaly_percentage']}%)\n"
                    elif 'territory' in anomaly:
                        prompt += f"  * –¢–µ—Ä—Ä–∏—Ç–æ—Ä–∏—è '{anomaly['territory']}': {anomaly['anomaly_count']} –∞–Ω–æ–º–∞–ª–∏–π ({anomaly['anomaly_percentage']}%)\n"
        prompt += "\n"
    
    prompt += "–°–æ–∑–¥–∞–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–µ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–µ —Å–∞–º–º–∞—Ä–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏."
    
    try:
        llm_response = call_giga_api_wrapper(prompt, system_instruction)
        return format_llm_summary(llm_response, analysis_context)
    except Exception as e:
        print(f"[LLM_SUMMARY ERROR] –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ LLM —Å–∞–º–º–∞—Ä–∏: {e}")
        return generate_fallback_summary_from_context(analysis_context)

def format_llm_summary(llm_response, analysis_context):
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç LLM –æ—Ç–≤–µ—Ç –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–∞–º–º–∞—Ä–∏"""
    # –°–æ–∑–¥–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
    analysis_type_name = "üîç –ê–Ω–∞–ª–∏–∑ –∞–Ω–æ–º–∞–ª–∏–π" if analysis_context['analysis_type'] == 'anomaly_detection' else "üìä –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö"
    
    formatted_summary = f"# {analysis_type_name}\n\n"
    formatted_summary += f"**–ó–∞–ø—Ä–æ—Å:** {analysis_context['original_query']}\n"
    formatted_summary += f"**–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞–ø–∏—Å–µ–π:** {analysis_context['total_records']}\n\n"
    
    # –î–æ–±–∞–≤–ª—è–µ–º LLM –∞–Ω–∞–ª–∏–∑
    formatted_summary += "## ü§ñ –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑\n\n"
    formatted_summary += llm_response + "\n\n"
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏ –µ—Å–ª–∏ –µ—Å—Ç—å –∞–Ω–æ–º–∞–ª–∏–∏
    if 'anomaly_analysis' in analysis_context and analysis_context['anomaly_analysis'].get('anomalies_found', False):
        formatted_summary += "## üìã –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏ –∞–Ω–æ–º–∞–ª–∏–π\n\n"
        anomaly_data = analysis_context['anomaly_analysis']
        
        if 'anomalies' in anomaly_data:
            for i, anomaly in enumerate(anomaly_data['anomalies'], 1):
                if 'column' in anomaly:
                    formatted_summary += f"**{i}. –°—Ç–æ–ª–±–µ—Ü `{anomaly['column']}`**\n"
                    formatted_summary += f"- –ê–Ω–æ–º–∞–ª–∏–π: {anomaly['anomaly_count']} –∏–∑ {anomaly['total_count']} ({anomaly['anomaly_percentage']}%)\n"
                    formatted_summary += f"- –î–∏–∞–ø–∞–∑–æ–Ω –Ω–æ—Ä–º—ã: {anomaly['bounds']['lower']:.2f} - {anomaly['bounds']['upper']:.2f}\n"
                    formatted_summary += f"- –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: Œº={anomaly['statistics']['mean']:.2f}, med={anomaly['statistics']['median']:.2f}, œÉ={anomaly['statistics']['std']:.2f}\n\n"
                elif 'territory' in anomaly:
                    formatted_summary += f"**{i}. –¢–µ—Ä—Ä–∏—Ç–æ—Ä–∏—è: {anomaly['territory']}**\n"
                    formatted_summary += f"- –ê–Ω–æ–º–∞–ª–∏–π: {anomaly['anomaly_count']} –∏–∑ {anomaly['total_count']} ({anomaly['anomaly_percentage']}%)\n"
                    formatted_summary += f"- Max Z-score: {anomaly['statistics']['max_z_score']:.2f}\n"
                    formatted_summary += f"- –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: Œº={anomaly['statistics']['mean']:.2f}, œÉ={anomaly['statistics']['std']:.2f}\n\n"
    
    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    formatted_summary += "---\n"
    formatted_summary += f"*–ê–Ω–∞–ª–∏–∑ –≤—ã–ø–æ–ª–Ω–µ–Ω —Å–∏—Å—Ç–µ–º–æ–π –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–æ–≥–æ –ò–ò*\n"
    
    return formatted_summary

def generate_fallback_summary_from_context(analysis_context):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∑–µ—Ä–≤–Ω–æ–µ —Å–∞–º–º–∞—Ä–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
    analysis_type_name = "üîç –ê–Ω–∞–ª–∏–∑ –∞–Ω–æ–º–∞–ª–∏–π" if analysis_context['analysis_type'] == 'anomaly_detection' else "üìä –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö"
    
    summary = f"# {analysis_type_name}\n\n"
    summary += f"**–ó–∞–ø—Ä–æ—Å:** {analysis_context['original_query']}\n"
    summary += f"**–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞–ø–∏—Å–µ–π:** {analysis_context['total_records']}\n\n"
    
    if analysis_context['data_available']:
        summary += "## üìã –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞\n\n"
        summary += f"–£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {analysis_context['total_records']} –∑–∞–ø–∏—Å–µ–π –¥–∞–Ω–Ω—ã—Ö.\n\n"
        
        if 'statistics' in analysis_context:
            summary += "**–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏:**\n"
            for col, stats in analysis_context['statistics'].items():
                summary += f"- `{col}`: —Å—Ä–µ–¥–Ω–µ–µ = {stats['mean']:.2f}, —Ä–∞–∑–±—Ä–æ—Å = {stats['min']:.2f} - {stats['max']:.2f}\n"
            summary += "\n"
    else:
        summary += "## ‚ö†Ô∏è –î–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã\n\n"
        summary += "–ü–æ —É–∫–∞–∑–∞–Ω–Ω—ã–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º –¥–∞–Ω–Ω—ã–µ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø–µ—Ä–µ—Å–º–æ—Ç—Ä–µ—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞.\n\n"
    
    if 'anomaly_analysis' in analysis_context:
        anomaly_data = analysis_context['anomaly_analysis']
        if anomaly_data.get('anomalies_found', False):
            summary += f"## üö® –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∞–Ω–æ–º–∞–ª–∏–∏\n\n"
            summary += f"–ù–∞–π–¥–µ–Ω–æ {anomaly_data.get('anomaly_count', 0)} —Ç–∏–ø–æ–≤ –∞–Ω–æ–º–∞–ª–∏–π –≤ –¥–∞–Ω–Ω—ã—Ö.\n\n"
        else:
            summary += f"## ‚úÖ –ê–Ω–æ–º–∞–ª–∏–∏ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã\n\n"
            summary += "–í—Å–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –Ω–æ—Ä–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π.\n\n"
    
    return summary

def generate_fallback_summary(sql_results_df, user_prompt, anomaly_results, needs_anomaly_detection):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Ä–æ—Å—Ç–æ–µ —Ä–µ–∑–µ—Ä–≤–Ω–æ–µ —Å–∞–º–º–∞—Ä–∏"""
    if sql_results_df is None:
        return "**‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞**\n\n–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –∑–∞–ø—Ä–æ—Å–∞."
    
    if sql_results_df.empty:
        return f"**üì≠ –î–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã**\n\n–ü–æ –∑–∞–ø—Ä–æ—Å—É '{user_prompt}' –¥–∞–Ω–Ω—ã–µ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å –∫—Ä–∏—Ç–µ—Ä–∏–∏ –ø–æ–∏—Å–∫–∞."
    
    summary = f"**üìä –ö—Ä–∞—Ç–∫–æ–µ —Å–∞–º–º–∞—Ä–∏**\n\n"
    summary += f"**–ó–∞–ø—Ä–æ—Å:** {user_prompt}\n"
    summary += f"**–ù–∞–π–¥–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π:** {len(sql_results_df)}\n"
    summary += f"**–°—Ç–æ–ª–±—Ü—ã –¥–∞–Ω–Ω—ã—Ö:** {', '.join(sql_results_df.columns.tolist())}\n\n"
    
    if needs_anomaly_detection and anomaly_results:
        if anomaly_results.get('anomalies_found', False):
            summary += f"**üö® –ê–Ω–æ–º–∞–ª–∏–∏:** –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {anomaly_results.get('anomaly_count', 0)} —Ç–∏–ø–æ–≤ –∞–Ω–æ–º–∞–ª–∏–π\n"
        else:
            summary += f"**‚úÖ –ê–Ω–æ–º–∞–ª–∏–∏:** –ù–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã\n"
    
    return summary

# ==============================================================================
# –û–°–ù–û–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø-–ì–ï–ù–ï–†–ê–¢–û–† (–û–ë–ù–û–í–õ–ï–ù–ù–ê–Ø)
# ==============================================================================

async def event_generator(user_prompt: str, duckdb_con):
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å–æ–±—ã—Ç–∏–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
    –í–∫–ª—é—á–∞–µ—Ç RAG, –∞–≥–µ–Ω—Ç–æ–≤, –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ SQL –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –æ—Ç–≤–µ—Ç–∞.
    """
    current_stage = "–ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏"
    current_data = {}
    rag_context_for_agents = None
    
    # Accumulated data for potential saving
    accumulated_data_for_rag = {
        "user_prompt": user_prompt,
        "formal_prompt": None,
        "plan": None,
        "sql_query": None,
        "final_answer": None
    }

    try:
        yield {"type": "status", "stage": "rag_check", "message": "–ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤..."}
        rag_result = agent_rag_retriever(user_prompt)
        rag_status = rag_result.get("status")
        rag_data = rag_result.get("data")

        if rag_status == "error":
            yield {"type": "error", "stage": "rag_check", "message": f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ—Å—Ç—É–ø–µ –∫ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π: {rag_result.get('message')}. –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ –Ω–µ–µ."}
        elif rag_status == "exact":
            final_answer = rag_data.get("final_answer", "–¢–æ—á–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞–π–¥–µ–Ω, –Ω–æ –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –µ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ.")
            accumulated_data_for_rag["formal_prompt"] = rag_data.get("formal_prompt")
            accumulated_data_for_rag["plan"] = rag_data.get("plan")
            accumulated_data_for_rag["sql_query"] = rag_data.get("sql_query")
            accumulated_data_for_rag["final_answer"] = final_answer
            yield {"type": "final_answer", "stage": "rag_exact_match", "answer": final_answer, "rag_components": accumulated_data_for_rag, "message": "–ù–∞–π–¥–µ–Ω —Ç–æ—á–Ω—ã–π –æ—Ç–≤–µ—Ç –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π."}
            print(f"[EVENT_GENERATOR] RAG exact match. Returning stored answer for: {user_prompt[:50]}")
            return
        elif rag_status == "similar":
            rag_context_for_agents = rag_data
            yield {"type": "status", "stage": "rag_similar_match", "message": "–ù–∞–π–¥–µ–Ω –ø–æ—Ö–æ–∂–∏–π –∑–∞–ø—Ä–æ—Å –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π. –ò—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞.", "similar_data_preview": {k: (v[:100] + '...' if isinstance(v, str) and len(v) > 100 else v) for k,v in rag_data.items()}}
            print(f"[EVENT_GENERATOR] RAG similar match found for: {user_prompt[:50]}")
        else:
            yield {"type": "status", "stage": "rag_no_match", "message": "–ü–æ—Ö–æ–∂–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ. –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞."}
            print(f"[EVENT_GENERATOR] RAG no match for: {user_prompt[:50]}")

        # 0. –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä (Anomaly detection check)
        current_stage = "–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∞–Ω–∞–ª–∏–∑–∞ (–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä)"
        yield {"type": "status", "stage": "coordinator", "message": current_stage + "..."}
        ambiguity_analysis_result, enhanced_prompt, needs_clarification, needs_anomaly_detection_local = await analyze_query_ambiguity(user_prompt)
        
        if needs_clarification:
            yield {"type": "clarification_needed", "stage": "ambiguity_analysis", "data": ambiguity_analysis_result}
            print(f"[EVENT_GENERATOR] Clarification needed for: {user_prompt[:50]}")
            return

        if enhanced_prompt and enhanced_prompt != user_prompt:
            yield {"type": "status", "stage": "ambiguity_analysis", "message": f"–ó–∞–ø—Ä–æ—Å –±—ã–ª –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —É–ª—É—á—à–µ–Ω: {enhanced_prompt}", "original_prompt": user_prompt, "enhanced_prompt": enhanced_prompt}
            processed_user_prompt = enhanced_prompt
        else:
            processed_user_prompt = user_prompt
        
        needs_anomaly_detection = needs_anomaly_detection_local

        # 1. –ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤—â–∏–∫
        current_stage = "–ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞"
        yield {"type": "status", "stage": "rephraser", "message": current_stage + "..."}
        formal_prompt_result = agent1_rephraser(processed_user_prompt, rag_context=rag_context_for_agents)
        accumulated_data_for_rag["formal_prompt"] = formal_prompt_result
        yield {"type": "intermediary_result", "stage": "rephraser", "name": "–§–æ—Ä–º–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å", "content": formal_prompt_result}

        # 2. –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫
        current_stage = "–°–æ–∑–¥–∞–Ω–∏–µ –ø–ª–∞–Ω–∞"
        yield {"type": "status", "stage": "planner", "message": current_stage + "..."}
        plan_result = agent2_planner(formal_prompt_result, rag_context=rag_context_for_agents)
        accumulated_data_for_rag["plan"] = plan_result
        yield {"type": "intermediary_result", "stage": "planner", "name": "–ü–ª–∞–Ω –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è", "content": plan_result}

        # 3. SQL –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä
        current_stage = "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è SQL"
        yield {"type": "status", "stage": "sql_generator", "message": current_stage + "..."}
        sql_query_result = agent3_sql_generator(plan_result, rag_context=rag_context_for_agents)
        yield {"type": "intermediary_result", "stage": "sql_generator", "name": "–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π SQL", "content": sql_query_result, "language": "sql"}

        # 4. –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ SQL
        current_stage = "–í–∞–ª–∏–¥–∞—Ü–∏—è SQL"
        max_fix_attempts = 3
        current_fix_attempt = 0
        validated_sql = None

        for attempt in range(max_fix_attempts):
            current_fix_attempt = attempt + 1
            yield {"type": "status", "stage": "sql_validation", "message": f"{current_stage} (–ø–æ–ø—ã—Ç–∫–∞ {current_fix_attempt}/{max_fix_attempts})..."}
            validator_result = agent_sql_validator(sql_query_result, plan_result)

            if validator_result.get("is_valid"):
                validated_sql = validator_result.get("corrected_sql") or sql_query_result
                yield {"type": "intermediary_result", "stage": "sql_validation", "name": "–í–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π SQL", "content": validated_sql, "language": "sql", "message": "SQL –ø—Ä–æ—à–µ–ª –≤–∞–ª–∏–¥–∞—Ü–∏—é."}
                break
            else:
                error_message = validator_result.get("message", "SQL –Ω–µ –ø—Ä–æ—à–µ–ª –≤–∞–ª–∏–¥–∞—Ü–∏—é.")
                yield {"type": "warning", "stage": "sql_validation", "message": f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ SQL: {error_message}"}
                
                if validator_result.get("corrected_sql"):
                    sql_query_result = validator_result["corrected_sql"]
                    yield {"type": "status", "stage": "sql_auto_correction", "message": "–í–∞–ª–∏–¥–∞—Ç–æ—Ä –ø—Ä–µ–¥–ª–æ–∂–∏–ª –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ. –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è...", "corrected_sql": sql_query_result}
                    continue

                current_stage = "–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ SQL (–∞–≥–µ–Ω—Ç–æ–º)"
                yield {"type": "status", "stage": "sql_fixer", "message": current_stage + "..."}
                fixed_sql = agent_sql_fixer(sql_query_result, error_message, plan_result)
                if fixed_sql and fixed_sql.strip():
                    sql_query_result = fixed_sql
                    yield {"type": "intermediary_result", "stage": "sql_fixer", "name": "–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π SQL (–∞–≥–µ–Ω—Ç–æ–º)", "content": sql_query_result, "language": "sql"}
                    current_stage = "–í–∞–ª–∏–¥–∞—Ü–∏—è SQL"
                else:
                    yield {"type": "error", "stage": "sql_fixer", "message": "–ê–≥–µ–Ω—Ç –Ω–µ —Å–º–æ–≥ –∏—Å–ø—Ä–∞–≤–∏—Ç—å SQL. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–ª–∞–Ω –∏–ª–∏ –∑–∞–ø—Ä–æ—Å."}
                    return

        if not validated_sql:
            yield {"type": "error", "stage": "sql_validation", "message": "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –≤–∞–ª–∏–¥–Ω—ã–π SQL –ø–æ—Å–ª–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ–ø—ã—Ç–æ–∫."}
            return
        
        accumulated_data_for_rag["sql_query"] = validated_sql

        # 5. –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ SQL
        current_stage = "–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ SQL –≤ DuckDB"
        yield {"type": "status", "stage": "sql_execution", "message": current_stage + "..."}
        try:
            sql_results_df = await execute_duckdb_query(duckdb_con, validated_sql)
            if sql_results_df is not None and not sql_results_df.empty:
                for col in sql_results_df.columns:
                    if pd.api.types.is_datetime64_any_dtype(sql_results_df[col]):
                        sql_results_df[col] = sql_results_df[col].astype(str)
                sql_results_json = sql_results_df.to_dict(orient='records')
                print(f"[EVENT_GENERATOR] SQL executed successfully. Rows returned: {len(sql_results_json)}")
            else:
                sql_results_json = []
                print(f"[EVENT_GENERATOR] SQL executed successfully but returned no data. Query was:\n{validated_sql}")
            yield {"type": "sql_result", "stage": "sql_execution", "data": sql_results_json, "row_count": len(sql_results_json), "message": "SQL –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ."}
        except Exception as e:
            yield {"type": "error", "stage": "sql_execution", "message": f"–û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è SQL: {str(e)}"}
            print(f"[EVENT_GENERATOR ERROR] SQL execution failed. Query was:\n{validated_sql}\nError: {str(e)}")
            current_stage = "–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ SQL –ø–æ—Å–ª–µ –æ—à–∏–±–∫–∏ –ë–î"
            yield {"type": "status", "stage": "sql_fixer_db_error", "message": current_stage + "..."}
            fixed_sql_db_error = agent_sql_fixer(validated_sql, str(e), plan_result)
            if fixed_sql_db_error and fixed_sql_db_error.strip():
                validated_sql = fixed_sql_db_error
                accumulated_data_for_rag["sql_query"] = validated_sql
                yield {"type": "intermediary_result", "stage": "sql_fixer_db_error", "name": "–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π SQL (–ø–æ—Å–ª–µ –æ—à–∏–±–∫–∏ –ë–î)", "content": validated_sql, "language": "sql"}
                yield {"type": "status", "stage": "sql_execution_retry", "message": "–ü–æ–≤—Ç–æ—Ä–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–≥–æ SQL..."}
                try:
                    sql_results_df = await execute_duckdb_query(duckdb_con, validated_sql)
                    if sql_results_df is not None and not sql_results_df.empty:
                        for col in sql_results_df.columns:
                            if pd.api.types.is_datetime64_any_dtype(sql_results_df[col]):
                                sql_results_df[col] = sql_results_df[col].astype(str)
                        sql_results_json = sql_results_df.to_dict(orient='records')
                    else:
                        sql_results_json = []
                    yield {"type": "sql_result", "stage": "sql_execution_retry", "data": sql_results_json, "row_count": len(sql_results_json), "message": "–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π SQL –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ."}
                except Exception as e_retry:
                    yield {"type": "error", "stage": "sql_execution_retry", "message": f"–û—à–∏–±–∫–∞ –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è SQL: {str(e_retry)}"}
                    return
            else:
                yield {"type": "error", "stage": "sql_fixer_db_error", "message": "–ê–≥–µ–Ω—Ç –Ω–µ —Å–º–æ–≥ –∏—Å–ø—Ä–∞–≤–∏—Ç—å SQL –ø–æ—Å–ª–µ –æ—à–∏–±–∫–∏ –ë–î."}
                return

        # 6. –ê–Ω–∞–ª–∏–∑ –∞–Ω–æ–º–∞–ª–∏–π (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
        anomaly_summary = None
        if needs_anomaly_detection:
            current_stage = "–ê–Ω–∞–ª–∏–∑ –∞–Ω–æ–º–∞–ª–∏–π"
            yield {"type": "status", "stage": "anomaly_detection", "message": current_stage + "..."}
            if sql_results_df is not None and not sql_results_df.empty:
                anomaly_results = agent_anomaly_detector(sql_results_df.copy(), processed_user_prompt)
                yield {"type": "intermediary_result", "stage": "anomaly_detection", "name": "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –∞–Ω–æ–º–∞–ª–∏–π", "content": anomaly_results}
                anomaly_summary = anomaly_results
            else:
                 yield {"type": "warning", "stage": "anomaly_detection", "message": "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∞–Ω–æ–º–∞–ª–∏–π."}

        # 7. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è LLM —Å–∞–º–º–∞—Ä–∏ / –æ—Ç–≤–µ—Ç–∞
        current_stage = "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç–≤–µ—Ç–∞"
        yield {"type": "status", "stage": "final_summary_generation", "message": current_stage + "..."}
        llm_summary_result = generate_llm_analysis_summary(
                sql_results_df, 
            processed_user_prompt, 
            anomaly_results=anomaly_summary,
            needs_anomaly_detection=needs_anomaly_detection,
            rag_context=rag_context_for_agents
        )
        accumulated_data_for_rag["final_answer"] = llm_summary_result
        yield {"type": "final_answer", "stage": "final_summary_generation", "answer": llm_summary_result, "rag_components": accumulated_data_for_rag, "message": "–ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω."}

    except Exception as e:
        error_message = f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞ —ç—Ç–∞–ø–µ '{current_stage}': {str(e)}"
        print(f"[EVENT_GENERATOR ERROR] {error_message}")
        import traceback
        traceback.print_exc()
        yield {"type": "error", "stage": current_stage, "message": error_message}
    finally:
        yield {"type": "status", "stage": "done", "message": "–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞."}

# ==============================================================================
# –≠–ù–î–ü–û–ò–ù–¢–´ (–ë–ï–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô)
# ==============================================================================

@app.get("/process_query_stream")
async def process_query_stream(user_prompt: str, request: Request):
    """Endpoint –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –ø–æ—Ç–æ–∫–æ–≤—ã–º –æ—Ç–≤–µ—Ç–æ–º"""
    duckdb_con = request.app.state.duckdb_con
    if not duckdb_con:
        return JSONResponse(
            status_code=500, 
            content={"error": "–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞."}
        )
    
    if not user_prompt or not user_prompt.strip():
        return JSONResponse(
            status_code=400, 
            content={"error": "user_prompt –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω –∏–ª–∏ –ø—É—Å—Ç."}
        )

    print(f"\n[API] –ü–æ–ª—É—á–µ–Ω –∑–∞–ø—Ä–æ—Å: {user_prompt}")

    async def format_sse(data: dict) -> str:
        """Format data as SSE message"""
        return f"data: {json.dumps(data)}\n\n"

    async def event_stream():
        async for event in event_generator(user_prompt.strip(), duckdb_con):
            yield await format_sse(event)

    return StreamingResponse(
        event_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "*",
            "Access-Control-Allow-Methods": "*"
        }
    )

@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"""
    try:
        if hasattr(app.state, 'duckdb_con') and app.state.duckdb_con:
            return {"status": "healthy", "service": "text2sql-multi-agent", "database": "connected"}
        else:
            return {"status": "unhealthy", "service": "text2sql-multi-agent", "database": "disconnected"}
    except Exception as e:
        return {"status": "unhealthy", "service": "text2sql-multi-agent", "error": str(e)}

@app.get("/tables")
async def get_available_tables():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü"""
    return {
        "tables": [
            {"name": "population", "description": "–î–∞–Ω–Ω—ã–µ –æ —á–∏—Å–ª–µ–Ω–Ω–æ—Å—Ç–∏ –Ω–∞—Å–µ–ª–µ–Ω–∏—è –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º", "type": "demographic"},
            {"name": "salary", "description": "–î–∞–Ω–Ω—ã–µ –æ –∑–∞—Ä–∞–±–æ—Ç–Ω—ã—Ö –ø–ª–∞—Ç–∞—Ö –ø–æ –æ—Ç—Ä–∞—Å–ª—è–º", "type": "economic"},
            {"name": "migration", "description": "–î–∞–Ω–Ω—ã–µ –æ –º–∏–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –ø–æ—Ç–æ–∫–∞—Ö", "type": "demographic"},
            {"name": "market_access", "description": "–ò–Ω–¥–µ–∫—Å—ã –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —Ä—ã–Ω–∫–æ–≤", "type": "economic"},
            {"name": "connections", "description": "–î–∞–Ω–Ω—ã–µ –æ —Å–≤—è–∑—è—Ö –º–µ–∂–¥—É —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏—è–º–∏", "type": "infrastructure"}
        ]
    }

# ==============================================================================
# EXCEPTION HANDLERS –ò STARTUP/SHUTDOWN (–ë–ï–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô)
# ==============================================================================

@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    """–ì–ª–æ–±–∞–ª—å–Ω—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –∏—Å–∫–ª—é—á–µ–Ω–∏–π"""
    print(f"[GLOBAL ERROR] –ù–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {exc}")
    return JSONResponse(
        status_code=500,
        content={
            "error": "–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞",
            "message": "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.",
            "type": "internal_server_error"
        }
    )

@app.on_event("startup")
async def startup_event():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ"""
    print("üöÄ –ó–∞–ø—É—Å–∫ Text2SQL Multi-Agent System with LLM Analysis...")
    
    try:
        duckdb_connection = setup_duckdb()
        if duckdb_connection is not None:
            app.state.duckdb_con = duckdb_connection
            print("‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö DuckDB –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
        else:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö")
            app.state.duckdb_con = None

        # Initialize RAG database
        print("Initializing RAG database for successful queries...")
        if not init_rag_db():
            print("[STARTUP ERROR] Failed to initialize RAG database. Application might not work as expected with RAG features.")
        else:
            print("RAG database initialized successfully.")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö: {e}")
        app.state.duckdb_con = None

@app.on_event("shutdown")
async def shutdown_event():
    """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤ –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏"""
    print("üõë –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã Text2SQL Multi-Agent System...")
    
    try:
        if (hasattr(app.state, 'duckdb_con') and 
            app.state.duckdb_con is not None and
            hasattr(app.state.duckdb_con, 'close')):
            app.state.duckdb_con.close()
            print("‚úÖ –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö –∑–∞–∫—Ä—ã—Ç–æ")
        else:
            print("‚ÑπÔ∏è –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö —É–∂–µ –∑–∞–∫—Ä—ã—Ç–æ –∏–ª–∏ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ")
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–∫—Ä—ã—Ç–∏–∏ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è: {e}")

# ==============================================================================
# –ó–ê–ü–£–°–ö –°–ï–†–í–ï–†–ê
# ==============================================================================

# Move RagSaveRequest definition here, before the endpoint that uses it.
class RagSaveRequest(BaseModel):
    user_prompt: str
    formal_prompt: str
    plan: str
    sql_query: str
    final_answer: str

@app.post("/llm_query/confirm_and_save")
async def confirm_answer_and_save_to_rag(payload: RagSaveRequest):
    """
    Endpoint to confirm a user liked an answer and save it to the RAG database.
    """
    print(f"[API /confirm_and_save] Received request to save: {payload.user_prompt[:50]}...")
    try:
        # Run the synchronous add_to_rag_db in a thread pool to avoid blocking the event loop
        loop = asyncio.get_event_loop()
        success = await loop.run_in_executor(None, lambda: add_to_rag_db(
            user_prompt=payload.user_prompt,
            formal_prompt=payload.formal_prompt,
            plan=payload.plan,
            sql_query=payload.sql_query,
            final_answer=payload.final_answer
        ))

        if success:
            print(f"[API /confirm_and_save] Successfully saved to RAG: {payload.user_prompt[:50]}")
            return JSONResponse(content={"status": "success", "message": "Answer saved to RAG successfully."}, status_code=200)
        else:
            print(f"[API /confirm_and_save] Failed to save to RAG: {payload.user_prompt[:50]}")
            raise HTTPException(status_code=500, detail="Failed to save answer to RAG database.")
    except Exception as e:
        print(f"[API /confirm_and_save] Error during RAG save: {e}")
        raise HTTPException(status_code=500, detail=f"An error occurred while saving to RAG: {str(e)}")

if __name__ == "__main__":
    print("üöÄ –ó–∞–ø—É—Å–∫ Text2SQL Multi-Agent System with LLM Analysis...")
    # Ensure the static directory and index.html are checked at startup for debugging
    if not os.path.isdir(STATIC_DIR):
        print(f"[CRITICAL ERROR] Static directory not found: {STATIC_DIR}")
    elif not os.path.exists(INDEX_HTML_PATH):
        print(f"[CRITICAL ERROR] index.html not found in static directory: {INDEX_HTML_PATH}")
    
    import uvicorn
    uvicorn.run(
        app, 
        host="0.0.0.0", 
        port=5002,
        reload=False, # Set to True for development if you want auto-reloading on code changes
        log_level="info",
        access_log=True
    )
