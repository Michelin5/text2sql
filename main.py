import asyncio
import json
import numpy as np
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse
import pandas as pd
from json.decoder import JSONDecodeError
from flask import Flask, request, jsonify
from vis_sber import visualize_report

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∞–≥–µ–Ω—Ç–æ–≤ –∏ —É—Ç–∏–ª–∏—Ç—ã
from giga_wrapper import giga_client, call_giga_api_wrapper
from agents import (
    agent0_coordinator,
    agent1_rephraser,
    agent2_planner,
    agent3_sql_generator,
    agent_sql_validator,
    agent_sql_fixer,
    agent_anomaly_detector
)
from duckdb_utils import setup_duckdb, execute_duckdb_query

app = FastAPI()

# CORS setup
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==============================================================================
# –û–ë–†–ê–ë–û–¢–ö–ê –ù–ï–û–ü–†–ï–î–ï–õ–ï–ù–ù–û–°–¢–ò –ò –£–¢–û–ß–ù–ï–ù–ò–ô
# ==============================================================================

async def analyze_query_ambiguity(user_prompt):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å –∑–∞–ø—Ä–æ—Å–∞ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä–∞"""
    prompt_lower = user_prompt.lower()

    # –î–µ—Ç–µ–∫—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π
    anomaly_keywords = ['–∞–Ω–æ–º–∞–ª–∏', '–≤—ã–±—Ä–æ—Å', '–Ω–µ–æ–±—ã—á–Ω', '—Å—Ç—Ä–∞–Ω–Ω', '–æ—Ç–∫–ª–æ–Ω–µ–Ω–∏', '—ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω', '–ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å']
    needs_anomaly = any(keyword in prompt_lower for keyword in anomaly_keywords)

    # –î–µ—Ç–µ–∫—Ü–∏—è —Ç–∞–±–ª–∏—Ü –≤ –∑–∞–ø—Ä–æ—Å–µ
    table_mentions = {
        'population': ['–Ω–∞—Å–µ–ª–µ–Ω', '–¥–µ–º–æ–≥—Ä–∞—Ñ', '–ª—é–¥–µ–π', '–∂–∏—Ç–µ–ª'],
        'salary': ['–∑–∞—Ä–ø–ª–∞—Ç', '–∑–∞—Ä–∞–±–æ—Ç–Ω', '–æ–∫–ª–∞–¥', '–¥–æ—Ö–æ–¥'],
        'migration': ['–º–∏–≥—Ä–∞', '–ø–µ—Ä–µ–µ–∑–¥', '–ø–µ—Ä–µ–º–µ—â–µ–Ω'],
        'market_access': ['—Ä—ã–Ω–æ–∫', '–¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç', '—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫'],
        'connections': ['—Å–≤—è–∑', '—Å–æ–µ–¥–∏–Ω–µ–Ω', '–º–∞—Ä—à—Ä—É—Ç']
    }

    mentioned_tables = []
    for table, keywords in table_mentions.items():
        if any(keyword in prompt_lower for keyword in keywords):
            mentioned_tables.append(table)

    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Ä–æ–≤–Ω—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏
    ambiguity_level = "low"
    needs_clarification = False
    enhanced_prompt = user_prompt

    # –í—ã—Å–æ–∫–∞—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å: –∞–Ω–æ–º–∞–ª–∏–∏ –±–µ–∑ —É–∫–∞–∑–∞–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã
    if (needs_anomaly and not mentioned_tables and len(user_prompt.split()) < 8):
        ambiguity_level = "high"
        needs_clarification = True

        clarification_data = {
            "needs_clarification": True,
            "ambiguity_level": ambiguity_level,
            "reason": "–ó–∞–ø—Ä–æ—Å –æ–± –∞–Ω–æ–º–∞–ª–∏—è—Ö –±–µ–∑ —É–∫–∞–∑–∞–Ω–∏—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã",
            "message": "–î–ª—è –ø–æ–∏—Å–∫–∞ –∞–Ω–æ–º–∞–ª–∏–π —É–∫–∞–∂–∏—Ç–µ, –≤ –∫–∞–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏—Å–∫–∞—Ç—å:",
            "suggested_tables": [
                {"id": "population", "name": "–ù–∞—Å–µ–ª–µ–Ω–∏–µ", "description": "–î–∞–Ω–Ω—ã–µ –æ —á–∏—Å–ª–µ–Ω–Ω–æ—Å—Ç–∏ –Ω–∞—Å–µ–ª–µ–Ω–∏—è –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º"},
                {"id": "salary", "name": "–ó–∞—Ä–ø–ª–∞—Ç—ã", "description": "–î–∞–Ω–Ω—ã–µ –æ –∑–∞—Ä–∞–±–æ—Ç–Ω—ã—Ö –ø–ª–∞—Ç–∞—Ö –ø–æ –æ—Ç—Ä–∞—Å–ª—è–º"},
                {"id": "migration", "name": "–ú–∏–≥—Ä–∞—Ü–∏—è", "description": "–î–∞–Ω–Ω—ã–µ –æ –º–∏–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –ø–æ—Ç–æ–∫–∞—Ö"},
                {"id": "market_access", "name": "–î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —Ä—ã–Ω–∫–æ–≤", "description": "–ò–Ω–¥–µ–∫—Å—ã –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —Ä—ã–Ω–∫–æ–≤"},
                {"id": "connections", "name": "–°–≤—è–∑–∏", "description": "–î–∞–Ω–Ω—ã–µ –æ —Å–≤—è–∑—è—Ö –º–µ–∂–¥—É —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏—è–º–∏"}
            ],
            "examples": [
                "–ù–∞–π–¥–∏ –∞–Ω–æ–º–∞–ª–∏–∏ –≤ –¥–∞–Ω–Ω—ã—Ö –æ –Ω–∞—Å–µ–ª–µ–Ω–∏–∏",
                "–ü–æ–∫–∞–∂–∏ –≤—ã–±—Ä–æ—Å—ã –≤ –∑–∞—Ä–ø–ª–∞—Ç–∞—Ö –ø–æ –æ—Ç—Ä–∞—Å–ª—è–º",
                "–û–±–Ω–∞—Ä—É–∂—å –Ω–µ–æ–±—ã—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –º–∏–≥—Ä–∞—Ü–∏–∏",
                "–ù–∞–π–¥–∏ –∞–Ω–æ–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —Ä—ã–Ω–∫–æ–≤"
            ]
        }
        return clarification_data, None, needs_clarification, needs_anomaly

    # –°—Ä–µ–¥–Ω—è—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ
    elif (needs_anomaly and not mentioned_tables):
        ambiguity_level = "medium"
        enhanced_prompt = f"–ù–∞–π–¥–∏ –∞–Ω–æ–º–∞–ª–∏–∏ –≤ –¥–∞–Ω–Ω—ã—Ö –æ –º–∏–≥—Ä–∞—Ü–∏–∏"

        enhancement_data = {
            "ambiguity_level": ambiguity_level,
            "original_prompt": user_prompt,
            "enhanced_prompt": enhanced_prompt,
            "reason": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±—Ä–∞–Ω–∞ —Ç–∞–±–ª–∏—Ü–∞ '–º–∏–≥—Ä–∞—Ü–∏—è' –¥–ª—è –ø–æ–∏—Å–∫–∞ –∞–Ω–æ–º–∞–ª–∏–π",
            "auto_selected_table": "migration"
        }
        return enhancement_data, enhanced_prompt, False, needs_anomaly

    # –ù–∏–∑–∫–∞—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å
    return None, enhanced_prompt, False, needs_anomaly

# ==============================================================================
# LLM-–ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –ò –ì–ï–ù–ï–†–ê–¶–ò–Ø –°–ê–ú–ú–ê–†–ò
# ==============================================================================

def generate_llm_analysis_summary(sql_results_df, user_prompt, anomaly_results=None, needs_anomaly_detection=False):
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç LLM-–∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —Å–∞–º–º–∞—Ä–∏
    """
    try:
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        analysis_context = prepare_analysis_context(sql_results_df, user_prompt, anomaly_results, needs_anomaly_detection)

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º LLM-–∞–Ω–∞–ª–∏–∑
        llm_summary = generate_intelligent_summary(analysis_context)

        return llm_summary
    except Exception as e:
        print(f"[LLM_ANALYSIS ERROR] –û—à–∏–±–∫–∞ –≤ LLM-–∞–Ω–∞–ª–∏–∑–µ: {e}")
        return generate_fallback_summary(sql_results_df, user_prompt, anomaly_results, needs_anomaly_detection)

def prepare_analysis_context(sql_results_df, user_prompt, anomaly_results, needs_anomaly_detection):
    """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è LLM-–∞–Ω–∞–ª–∏–∑–∞"""
    context = {
        "original_query": user_prompt,
        "data_available": sql_results_df is not None and not sql_results_df.empty,
        "total_records": len(sql_results_df) if sql_results_df is not None else 0,
        "analysis_type": "anomaly_detection" if needs_anomaly_detection else "standard_analysis"
    }

    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞–Ω–Ω—ã—Ö
    if sql_results_df is not None and not sql_results_df.empty:
        context["columns"] = list(sql_results_df.columns)
        context["data_types"] = {col: str(dtype) for col, dtype in sql_results_df.dtypes.items()}

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —á–∏—Å–ª–æ–≤—ã–º —Å—Ç–æ–ª–±—Ü–∞–º
        numeric_cols = sql_results_df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            context["statistics"] = {}
            for col in numeric_cols:
                col_data = sql_results_df[col].dropna()
                if len(col_data) > 0:
                    context["statistics"][col] = {
                        "mean": float(col_data.mean()),
                        "median": float(col_data.median()),
                        "std": float(col_data.std()),
                        "min": float(col_data.min()),
                        "max": float(col_data.max())
                    }

        # –û–±—Ä–∞–∑–µ—Ü –¥–∞–Ω–Ω—ã—Ö
        context["sample_data"] = sql_results_df.head(5).to_dict('records')

    # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –∞–Ω–æ–º–∞–ª–∏–π
    if anomaly_results:
        context["anomaly_analysis"] = anomaly_results

    return context

def generate_intelligent_summary(analysis_context):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ —Å–∞–º–º–∞—Ä–∏ —Å –ø–æ–º–æ—â—å—é LLM"""
    system_instruction = """
–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç-–∞–Ω–∞–ª–∏—Ç–∏–∫ –¥–∞–Ω–Ω—ã—Ö. –ù–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞ —Å–æ–∑–¥–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ, –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–µ —Å–∞–º–º–∞—Ä–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.

–¢–≤–æ—è –∑–∞–¥–∞—á–∞:
1. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
2. –í—ã–¥–µ–ª–∏—Ç—å –∫–ª—é—á–µ–≤—ã–µ –∏–Ω—Å–∞–π—Ç—ã –∏ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
3. –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å actionable —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
4. –°—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤—ã–≤–æ–¥—ã –≤ –ø–æ–Ω—è—Ç–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ

–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç–≤–µ—Ç–∞ –¥–æ–ª–∂–Ω–∞ –≤–∫–ª—é—á–∞—Ç—å:
- –ö—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
- –ö–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã (—Å–ø–∏—Å–æ–∫)
- –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –∏–Ω—Å–∞–π—Ç—ã (–µ—Å–ª–∏ –ø—Ä–∏–º–µ–Ω–∏–º–æ)
- –û–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ –∞–Ω–æ–º–∞–ª–∏–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
- –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ (–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã)
- –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∏ –ø—Ä–µ–¥–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω–∏—è (–µ—Å–ª–∏ –µ—Å—Ç—å)

–ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π, –Ω–æ –¥–æ—Å—Ç—É–ø–Ω—ã–π —è–∑—ã–∫. –ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –∏ —Ñ–æ–∫—É—Å–∏—Ä—É–π—Å—è –Ω–∞ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–π —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.
"""

    prompt = f"""
–ö–æ–Ω—Ç–µ–∫—Å—Ç –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö:

–ò—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: "{analysis_context['original_query']}"

–¢–∏–ø –∞–Ω–∞–ª–∏–∑–∞: {analysis_context['analysis_type']}
–î–∞–Ω–Ω—ã–µ –¥–æ—Å—Ç—É–ø–Ω—ã: {'–î–∞' if analysis_context['data_available'] else '–ù–µ—Ç'}
–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {analysis_context['total_records']}

"""

    if analysis_context['data_available']:
        prompt += f"""
–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö:
- –°—Ç–æ–ª–±—Ü—ã: {', '.join(analysis_context['columns'])}
- –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö: {analysis_context['data_types']}

"""

        if 'statistics' in analysis_context:
            prompt += "–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏:\n"
            for col, stats in analysis_context['statistics'].items():
                prompt += f"- {col}: —Å—Ä–µ–¥–Ω–µ–µ={stats['mean']:.2f}, –º–µ–¥–∏–∞–Ω–∞={stats['median']:.2f}, œÉ={stats['std']:.2f}, –º–∏–Ω={stats['min']:.2f}, –º–∞–∫—Å={stats['max']:.2f}\n"
            prompt += "\n"

        if 'sample_data' in analysis_context:
            prompt += f"–û–±—Ä–∞–∑–µ—Ü –¥–∞–Ω–Ω—ã—Ö (–ø–µ—Ä–≤—ã–µ 5 –∑–∞–ø–∏—Å–µ–π):\n{analysis_context['sample_data']}\n\n"

    if 'anomaly_analysis' in analysis_context:
        anomaly_data = analysis_context['anomaly_analysis']
        prompt += f"""
–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –∞–Ω–æ–º–∞–ª–∏–π:
- –ê–Ω–æ–º–∞–ª–∏–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã: {'–î–∞' if anomaly_data.get('anomalies_found', False) else '–ù–µ—Ç'}
"""
        if anomaly_data.get('anomalies_found', False):
            prompt += f"- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–∏–ø–æ–≤ –∞–Ω–æ–º–∞–ª–∏–π: {anomaly_data.get('anomaly_count', 0)}\n"
            if 'anomalies' in anomaly_data:
                prompt += "–î–µ—Ç–∞–ª–∏ –∞–Ω–æ–º–∞–ª–∏–π:\n"
                for anomaly in anomaly_data['anomalies']:
                    if 'column' in anomaly:
                        prompt += f"  * –°—Ç–æ–ª–±–µ—Ü '{anomaly['column']}': {anomaly['anomaly_count']} –∞–Ω–æ–º–∞–ª–∏–π ({anomaly['anomaly_percentage']}%)\n"
                    elif 'territory' in anomaly:
                        prompt += f"  * –¢–µ—Ä—Ä–∏—Ç–æ—Ä–∏—è '{anomaly['territory']}': {anomaly['anomaly_count']} –∞–Ω–æ–º–∞–ª–∏–π ({anomaly['anomaly_percentage']}%)\n"
        prompt += "\n"

    prompt += "–°–æ–∑–¥–∞–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–µ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–µ —Å–∞–º–º–∞—Ä–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏."

    try:
        llm_response = call_giga_api_wrapper(prompt, system_instruction)
        return format_llm_summary(llm_response, analysis_context)
    except Exception as e:
        print(f"[LLM_SUMMARY ERROR] –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ LLM —Å–∞–º–º–∞—Ä–∏: {e}")
        return generate_fallback_summary_from_context(analysis_context)

def format_llm_summary(llm_response, analysis_context):
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç LLM –æ—Ç–≤–µ—Ç –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–∞–º–º–∞—Ä–∏"""
    # –°–æ–∑–¥–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
    analysis_type_name = "üîç –ê–Ω–∞–ª–∏–∑ –∞–Ω–æ–º–∞–ª–∏–π" if analysis_context['analysis_type'] == 'anomaly_detection' else "üìä –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö"

    formatted_summary = f"# {analysis_type_name}\n\n"
    formatted_summary += f"**–ó–∞–ø—Ä–æ—Å:** {analysis_context['original_query']}\n"
    formatted_summary += f"**–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞–ø–∏—Å–µ–π:** {analysis_context['total_records']}\n\n"

    # –î–æ–±–∞–≤–ª—è–µ–º LLM –∞–Ω–∞–ª–∏–∑
    formatted_summary += "## ü§ñ –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑\n\n"
    formatted_summary += llm_response + "\n\n"

    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏ –µ—Å–ª–∏ –µ—Å—Ç—å –∞–Ω–æ–º–∞–ª–∏–∏
    if 'anomaly_analysis' in analysis_context and analysis_context['anomaly_analysis'].get('anomalies_found', False):
        formatted_summary += "## üìã –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏ –∞–Ω–æ–º–∞–ª–∏–π\n\n"
        anomaly_data = analysis_context['anomaly_analysis']

        if 'anomalies' in anomaly_data:
            for i, anomaly in enumerate(anomaly_data['anomalies'], 1):
                if 'column' in anomaly:
                    formatted_summary += f"**{i}. –°—Ç–æ–ª–±–µ—Ü `{anomaly['column']}`**\n"
                    formatted_summary += f"- –ê–Ω–æ–º–∞–ª–∏–π: {anomaly['anomaly_count']} –∏–∑ {anomaly['total_count']} ({anomaly['anomaly_percentage']}%)\n"
                    formatted_summary += f"- –î–∏–∞–ø–∞–∑–æ–Ω –Ω–æ—Ä–º—ã: {anomaly['bounds']['lower']:.2f} - {anomaly['bounds']['upper']:.2f}\n"
                    formatted_summary += f"- –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: Œº={anomaly['statistics']['mean']:.2f}, med={anomaly['statistics']['median']:.2f}, œÉ={anomaly['statistics']['std']:.2f}\n\n"
                elif 'territory' in anomaly:
                    formatted_summary += f"**{i}. –¢–µ—Ä—Ä–∏—Ç–æ—Ä–∏—è: {anomaly['territory']}**\n"
                    formatted_summary += f"- –ê–Ω–æ–º–∞–ª–∏–π: {anomaly['anomaly_count']} –∏–∑ {anomaly['total_count']} ({anomaly['anomaly_percentage']}%)\n"
                    formatted_summary += f"- Max Z-score: {anomaly['statistics']['max_z_score']:.2f}\n"
                    formatted_summary += f"- –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: Œº={anomaly['statistics']['mean']:.2f}, œÉ={anomaly['statistics']['std']:.2f}\n\n"

    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    formatted_summary += "---\n"
    formatted_summary += f"*–ê–Ω–∞–ª–∏–∑ –≤—ã–ø–æ–ª–Ω–µ–Ω —Å–∏—Å—Ç–µ–º–æ–π –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–æ–≥–æ –ò–ò*\n"

    return formatted_summary

def generate_fallback_summary_from_context(analysis_context):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∑–µ—Ä–≤–Ω–æ–µ —Å–∞–º–º–∞—Ä–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
    analysis_type_name = "üîç –ê–Ω–∞–ª–∏–∑ –∞–Ω–æ–º–∞–ª–∏–π" if analysis_context['analysis_type'] == 'anomaly_detection' else "üìä –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö"

    summary = f"# {analysis_type_name}\n\n"
    summary += f"**–ó–∞–ø—Ä–æ—Å:** {analysis_context['original_query']}\n"
    summary += f"**–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞–ø–∏—Å–µ–π:** {analysis_context['total_records']}\n\n"

    if analysis_context['data_available']:
        summary += "## üìã –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞\n\n"
        summary += f"–£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {analysis_context['total_records']} –∑–∞–ø–∏—Å–µ–π –¥–∞–Ω–Ω—ã—Ö.\n\n"

        if 'statistics' in analysis_context:
            summary += "**–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏:**\n"
            for col, stats in analysis_context['statistics'].items():
                summary += f"- `{col}`: —Å—Ä–µ–¥–Ω–µ–µ = {stats['mean']:.2f}, —Ä–∞–∑–±—Ä–æ—Å = {stats['min']:.2f} - {stats['max']:.2f}\n"
            summary += "\n"
    else:
        summary += "## ‚ö†Ô∏è –î–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã\n\n"
        summary += "–ü–æ —É–∫–∞–∑–∞–Ω–Ω—ã–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º –¥–∞–Ω–Ω—ã–µ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø–µ—Ä–µ—Å–º–æ—Ç—Ä–µ—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞.\n\n"

    if 'anomaly_analysis' in analysis_context:
        anomaly_data = analysis_context['anomaly_analysis']
        if anomaly_data.get('anomalies_found', False):
            summary += f"## üö® –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∞–Ω–æ–º–∞–ª–∏–∏\n\n"
            summary += f"–ù–∞–π–¥–µ–Ω–æ {anomaly_data.get('anomaly_count', 0)} —Ç–∏–ø–æ–≤ –∞–Ω–æ–º–∞–ª–∏–π –≤ –¥–∞–Ω–Ω—ã—Ö.\n\n"
        else:
            summary += f"## ‚úÖ –ê–Ω–æ–º–∞–ª–∏–∏ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã\n\n"
            summary += "–í—Å–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –Ω–æ—Ä–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π.\n\n"

    return summary

def generate_fallback_summary(sql_results_df, user_prompt, anomaly_results, needs_anomaly_detection):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Ä–æ—Å—Ç–æ–µ —Ä–µ–∑–µ—Ä–≤–Ω–æ–µ —Å–∞–º–º–∞—Ä–∏"""
    if sql_results_df is None:
        return "**‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞**\n\n–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –∑–∞–ø—Ä–æ—Å–∞."

    if sql_results_df.empty:
        return f"**üì≠ –î–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã**\n\n–ü–æ –∑–∞–ø—Ä–æ—Å—É '{user_prompt}' –¥–∞–Ω–Ω—ã–µ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å –∫—Ä–∏—Ç–µ—Ä–∏–∏ –ø–æ–∏—Å–∫–∞."

    summary = f"**üìä –ö—Ä–∞—Ç–∫–æ–µ —Å–∞–º–º–∞—Ä–∏**\n\n"
    summary += f"**–ó–∞–ø—Ä–æ—Å:** {user_prompt}\n"
    summary += f"**–ù–∞–π–¥–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π:** {len(sql_results_df)}\n"
    summary += f"**–°—Ç–æ–ª–±—Ü—ã –¥–∞–Ω–Ω—ã—Ö:** {', '.join(sql_results_df.columns.tolist())}\n\n"

    if needs_anomaly_detection and anomaly_results:
        if anomaly_results.get('anomalies_found', False):
            summary += f"**üö® –ê–Ω–æ–º–∞–ª–∏–∏:** –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {anomaly_results.get('anomaly_count', 0)} —Ç–∏–ø–æ–≤ –∞–Ω–æ–º–∞–ª–∏–π\n"
        else:
            summary += f"**‚úÖ –ê–Ω–æ–º–∞–ª–∏–∏:** –ù–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã\n"

    return summary

# ==============================================================================
# –û–°–ù–û–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø-–ì–ï–ù–ï–†–ê–¢–û–† (–û–ë–ù–û–í–õ–ï–ù–ù–ê–Ø)
# ==============================================================================

async def event_generator(user_prompt: str, duckdb_con):
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å LLM-–∞–Ω–∞–ª–∏–∑–æ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    """
    anomaly_results = None  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –∞–Ω–æ–º–∞–ª–∏–π

    try:
        print(f"\n[Stream] –ü–æ–ª—É—á–µ–Ω –∑–∞–ø—Ä–æ—Å: {user_prompt}")

        # --- –ê–Ω–∞–ª–∏–∑ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ (–≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π) ---
        ambiguity_data, enhanced_prompt, needs_clarification, needs_anomaly_detection = await analyze_query_ambiguity(user_prompt)

        if needs_clarification:
            yield f"data: {json.dumps({'step': 'clarification_needed', 'content': ambiguity_data}, ensure_ascii=False)}\n\n"
            yield f"data: {json.dumps({'step': 'done', 'content': ''}, ensure_ascii=False)}\n\n"
            return

        if ambiguity_data and enhanced_prompt != user_prompt:
            yield f"data: {json.dumps({'step': 'query_enhanced', 'content': ambiguity_data}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.01)
            user_prompt = enhanced_prompt
            print(f"[Stream] –ó–∞–ø—Ä–æ—Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —É–ª—É—á—à–µ–Ω: {user_prompt}")

        # --- –ê–≥–µ–Ω—Ç 0: –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä ---
        try:
            coordination_result = agent0_coordinator(user_prompt)

            if isinstance(coordination_result, str):
                print(f"[COORDINATOR WARNING] –ü–æ–ª—É—á–µ–Ω–∞ —Å—Ç—Ä–æ–∫–∞: {coordination_result}")
                try:
                    import re
                    json_match = re.search(r'\{.*\}', coordination_result, re.DOTALL)
                    if json_match:
                        coordination_result = json.loads(json_match.group(0))
                    else:
                        raise ValueError("JSON –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –æ—Ç–≤–µ—Ç–µ")
                except Exception as parse_error:
                    print(f"[COORDINATOR ERROR] –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {parse_error}")
                    coordination_result = {
                        "needs_anomaly_detection": needs_anomaly_detection,
                        "analysis_type": "anomaly" if needs_anomaly_detection else "standard",
                        "keywords": []
                    }

            if coordination_result.get("needs_anomaly_detection") is not None:
                needs_anomaly_detection = coordination_result.get("needs_anomaly_detection", False)

            print(f"[Stream] –†–µ–∑—É–ª—å—Ç–∞—Ç –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏–∏: {coordination_result}")
            yield f"data: {json.dumps({'step': 'coordination', 'content': coordination_result}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.01)

        except Exception as coord_error:
            print(f"[COORDINATOR ERROR] –û—à–∏–±–∫–∞ –≤ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä–µ: {coord_error}")
            coordination_result = {
                "needs_anomaly_detection": needs_anomaly_detection,
                "analysis_type": "anomaly" if needs_anomaly_detection else "standard",
                "keywords": [],
                "error": str(coord_error)
            }
            yield f"data: {json.dumps({'step': 'coordination', 'content': coordination_result}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.01)

        # --- –ê–≥–µ–Ω—Ç 1: –§–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞ ---
        try:
            formal_request = agent1_rephraser(user_prompt)
            print(f"[Stream] –§–æ—Ä–º–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å: {formal_request}")
            yield f"data: {json.dumps({'step': 'formal_request', 'content': formal_request}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.01)
        except Exception as e:
            print(f"[REPHRASER ERROR] –û—à–∏–±–∫–∞ –≤ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}")
            formal_request = user_prompt
            yield f"data: {json.dumps({'step': 'formal_request', 'content': formal_request}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.01)

        # --- –ê–≥–µ–Ω—Ç 2: –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø–ª–∞–Ω–∞ ---
        try:
            plan = agent2_planner(formal_request)
            print(f"[Stream] –ü–ª–∞–Ω: {plan}")
            yield f"data: {json.dumps({'step': 'plan', 'content': plan}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.01)
        except Exception as e:
            print(f"[PLANNER ERROR] –û—à–∏–±–∫–∞ –≤ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}")
            plan = f"1. –ê–Ω–∞–ª–∏–∑ –∑–∞–ø—Ä–æ—Å–∞: {formal_request}\n2. –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n3. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞"
            yield f"data: {json.dumps({'step': 'plan', 'content': plan}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.01)

        # --- –ê–≥–µ–Ω—Ç 3: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è SQL ---
        try:
            generated_sql_query = agent3_sql_generator(plan)
            print(f"[Stream] –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π SQL: {generated_sql_query}")
            yield f"data: {json.dumps({'step': 'generated_sql_query', 'content': generated_sql_query}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.01)
        except Exception as e:
            print(f"[SQL_GENERATOR ERROR] –û—à–∏–±–∫–∞ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ SQL: {e}")
            yield f"data: {json.dumps({'step': 'error', 'content': f'–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ SQL: {str(e)}'}, ensure_ascii=False)}\n\n"
            yield f"data: {json.dumps({'step': 'done', 'content': ''}, ensure_ascii=False)}\n\n"
            return

        current_sql_query = generated_sql_query
        sql_results_df = None
        MAX_FIX_ATTEMPTS = 2

        # --- –¶–∏–∫–ª –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è SQL ---
        for attempt in range(MAX_FIX_ATTEMPTS):
            if not current_sql_query or "SELECT" not in current_sql_query.upper():
                error_msg = '–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π SQL-–∑–∞–ø—Ä–æ—Å. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É –∑–∞–ø—Ä–æ—Å–∞.'
                yield f"data: {json.dumps({'step': 'final_answer', 'content': error_msg}, ensure_ascii=False)}\n\n"
                break

            yield f"data: {json.dumps({'step': 'executed_sql_query', 'content': current_sql_query}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.01)

            try:
                print(f"[Stream] –ü–æ–ø—ã—Ç–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è SQL ({attempt + 1}/{MAX_FIX_ATTEMPTS})...")
                sql_results_df = execute_duckdb_query(current_sql_query)

                if sql_results_df is not None and not sql_results_df.empty:
                    sql_results_str = sql_results_df.to_markdown(index=False)
                    print(f"[Stream] SQL –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ. –ü–æ–ª—É—á–µ–Ω–æ {len(sql_results_df)} —Å—Ç—Ä–æ–∫.")
                else:
                    sql_results_str = "–ó–∞–ø—Ä–æ—Å SQL –≤—ã–ø–æ–ª–Ω–µ–Ω, –Ω–æ –¥–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã."
                    print("[Stream] SQL –≤—ã–ø–æ–ª–Ω–µ–Ω, –Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—É—Å—Ç–æ–π.")

                yield f"data: {json.dumps({'step': 'sql_results_str', 'content': sql_results_str}, ensure_ascii=False)}\n\n"
                await asyncio.sleep(0.01)
                break



            except Exception as e:
                sql_error_message = str(e)
                print(f"[Stream] –û—à–∏–±–∫–∞ DuckDB: {sql_error_message}")
                yield f"data: {json.dumps({'step': 'sql_error', 'content': sql_error_message}, ensure_ascii=False)}\n\n"
                await asyncio.sleep(0.01)

                if attempt < MAX_FIX_ATTEMPTS - 1:
                    print("[Stream] –ü–æ–ø—ã—Ç–∫–∞ –∏—Å–ø—Ä–∞–≤–∏—Ç—å SQL...")
                    try:
                        fixed_sql = agent_sql_fixer(current_sql_query, sql_error_message, plan)
                        fix_log_entry = {
                            "fix_attempt": attempt + 1,
                            "fixed_sql": fixed_sql,
                            "original_sql": current_sql_query,
                            "error": sql_error_message
                        }
                        yield f"data: {json.dumps({'step': 'sql_validation_log', 'content': fix_log_entry}, ensure_ascii=False)}\n\n"
                        await asyncio.sleep(0.01)

                        if fixed_sql and fixed_sql.strip():
                            print(f"[Stream] Fixer –ø—Ä–µ–¥–ª–æ–∂–∏–ª –Ω–æ–≤—ã–π SQL: {fixed_sql}")
                            current_sql_query = fixed_sql
                        else:
                            print("[Stream] Fixer –Ω–µ —Å–º–æ–≥ –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ.")
                            sql_results_df = None
                            break
                    except Exception as fixer_error:
                        print(f"[FIXER ERROR] –û—à–∏–±–∫–∞ –≤ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ SQL: {fixer_error}")
                        sql_results_df = None
                        break
                else:
                    print("[Stream] –î–æ—Å—Ç–∏–≥–Ω—É—Ç–æ –º–∞–∫—Å. —á–∏—Å–ª–æ –ø–æ–ø—ã—Ç–æ–∫ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è.")
                    sql_results_df = None

        # --- –ê–≥–µ–Ω—Ç –∞–Ω–æ–º–∞–ª–∏–π (–µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è) ---
        if needs_anomaly_detection:
            print("[Stream] –ó–∞–ø—É—Å–∫ –∞–≥–µ–Ω—Ç–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π...")
            try:
                anomaly_results = agent_anomaly_detector(sql_results_df, user_prompt)
                print(f"[Stream] –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –∞–Ω–æ–º–∞–ª–∏–π: {anomaly_results}")
                yield f"data: {json.dumps({'step': 'anomaly_analysis', 'content': anomaly_results}, ensure_ascii=False)}\n\n"
                await asyncio.sleep(0.01)
            except Exception as e:
                print(f"[ANOMALY ERROR] –û—à–∏–±–∫–∞ –≤ –∞–≥–µ–Ω—Ç–µ –∞–Ω–æ–º–∞–ª–∏–π: {e}")
                anomaly_results = {
                    "anomalies_found": False,
                    "error": f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∞–Ω–æ–º–∞–ª–∏–π: {str(e)}",
                    "message": "–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∞–Ω–∞–ª–∏–∑ –∞–Ω–æ–º–∞–ª–∏–π"
                }
                yield f"data: {json.dumps({'step': 'anomaly_analysis', 'content': anomaly_results}, ensure_ascii=False)}\n\n"
                await asyncio.sleep(0.01)

        # --- LLM-–ê–ù–ê–õ–ò–ó –ò –ì–ï–ù–ï–†–ê–¶–ò–Ø –°–ê–ú–ú–ê–†–ò ---
        print("[Stream] –ì–µ–Ω–µ—Ä–∞—Ü–∏—è LLM-—Å–∞–º–º–∞—Ä–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
        yield f"data: {json.dumps({'step': 'llm_analysis_start', 'content': '–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ —Å–∞–º–º–∞—Ä–∏...'}, ensure_ascii=False)}\n\n"
        await asyncio.sleep(0.01)

        try:
            llm_summary = generate_llm_analysis_summary(
                sql_results_df,
                user_prompt,
                anomaly_results,
                needs_anomaly_detection
            )
            print("[Stream] LLM-—Å–∞–º–º–∞—Ä–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ.")
            yield f"data: {json.dumps({'step': 'final_answer', 'content': llm_summary}, ensure_ascii=False)}\n\n"
        except Exception as e:
            print(f"[LLM_SUMMARY ERROR] –û—à–∏–±–∫–∞ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ LLM-—Å–∞–º–º–∞—Ä–∏: {e}")
            fallback_summary = generate_fallback_summary(sql_results_df, user_prompt, anomaly_results, needs_anomaly_detection)
            yield f"data: {json.dumps({'step': 'final_answer', 'content': fallback_summary}, ensure_ascii=False)}\n\n"

    except Exception as e:
        print(f"[Stream] –ü—Ä–æ–∏–∑–æ—à–ª–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–µ: {e}")
        error_content = f"**‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ —Å–∏—Å—Ç–µ–º—ã**\n\n–ü—Ä–æ–∏–∑–æ—à–ª–∞ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}\n\n–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑ –∏–ª–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É."
        yield f"data: {json.dumps({'step': 'error', 'content': error_content}, ensure_ascii=False)}\n\n"

    finally:
        print("[Stream] –û—Ç–ø—Ä–∞–≤–∫–∞ —Å–∏–≥–Ω–∞–ª–∞ –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏.")
        yield f"data: {json.dumps({'step': 'done', 'content': ''}, ensure_ascii=False)}\n\n"

# ==============================================================================
# –≠–ù–î–ü–û–ò–ù–¢–´ (–ë–ï–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô)
# ==============================================================================

@app.get("/process_query_stream")
async def process_query_stream(user_prompt: str, request: Request):
    """Endpoint –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –ø–æ—Ç–æ–∫–æ–≤—ã–º –æ—Ç–≤–µ—Ç–æ–º"""
    duckdb_con = request.app.state.duckdb_con
    if not duckdb_con:
        return JSONResponse(
            status_code=500,
            content={"error": "–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞."}
        )

    if not user_prompt or not user_prompt.strip():
        return JSONResponse(
            status_code=400,
            content={"error": "user_prompt –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω –∏–ª–∏ –ø—É—Å—Ç."}
        )

    print(f"\n[API] –ü–æ–ª—É—á–µ–Ω –∑–∞–ø—Ä–æ—Å: {user_prompt}")

    return StreamingResponse(
        event_generator(user_prompt.strip(), duckdb_con),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "*",
            "Access-Control-Allow-Methods": "*"
        }
    )

@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"""
    try:
        if hasattr(app.state, 'duckdb_con') and app.state.duckdb_con:
            return {"status": "healthy", "service": "text2sql-multi-agent", "database": "connected"}
        else:
            return {"status": "unhealthy", "service": "text2sql-multi-agent", "database": "disconnected"}
    except Exception as e:
        return {"status": "unhealthy", "service": "text2sql-multi-agent", "error": str(e)}

@app.get("/")
async def root():
    """–ö–æ—Ä–Ω–µ–≤–æ–π —ç–Ω–¥–ø–æ–∏–Ω—Ç"""
    return {
        "message": "Text2SQL Multi-Agent System with LLM-Enhanced Analysis",
        "version": "2.3.0",
        "features": [
            "Intelligent query coordination",
            "Advanced anomaly detection with LLM analysis",
            "Automated ambiguity resolution",
            "Multi-agent processing pipeline",
            "Real-time streaming responses",
            "LLM-generated structured summaries"
        ],
        "supported_queries": [
            "Standard data analysis with intelligent summaries",
            "Anomaly detection with expert-level insights",
            "Statistical analysis with actionable recommendations",
            "Data exploration with comprehensive reporting"
        ]
    }

@app.get("/tables")
async def get_available_tables():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü"""
    return {
        "tables": [
            {"name": "population", "description": "–î–∞–Ω–Ω—ã–µ –æ —á–∏—Å–ª–µ–Ω–Ω–æ—Å—Ç–∏ –Ω–∞—Å–µ–ª–µ–Ω–∏—è –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º", "type": "demographic"},
            {"name": "salary", "description": "–î–∞–Ω–Ω—ã–µ –æ –∑–∞—Ä–∞–±–æ—Ç–Ω—ã—Ö –ø–ª–∞—Ç–∞—Ö –ø–æ –æ—Ç—Ä–∞—Å–ª—è–º", "type": "economic"},
            {"name": "migration", "description": "–î–∞–Ω–Ω—ã–µ –æ –º–∏–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –ø–æ—Ç–æ–∫–∞—Ö", "type": "demographic"},
            {"name": "market_access", "description": "–ò–Ω–¥–µ–∫—Å—ã –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —Ä—ã–Ω–∫–æ–≤", "type": "economic"},
            {"name": "connections", "description": "–î–∞–Ω–Ω—ã–µ –æ —Å–≤—è–∑—è—Ö –º–µ–∂–¥—É —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏—è–º–∏", "type": "infrastructure"}
        ]
    }

# ==============================================================================
# EXCEPTION HANDLERS –ò STARTUP/SHUTDOWN (–ë–ï–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô)
# ==============================================================================

@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    """–ì–ª–æ–±–∞–ª—å–Ω—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –∏—Å–∫–ª—é—á–µ–Ω–∏–π"""
    print(f"[GLOBAL ERROR] –ù–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {exc}")
    return JSONResponse(
        status_code=500,
        content={
            "error": "–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞",
            "message": "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.",
            "type": "internal_server_error"
        }
    )

@app.on_event("startup")
async def startup_event():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ"""
    print("üöÄ –ó–∞–ø—É—Å–∫ Text2SQL Multi-Agent System with LLM Analysis...")

    try:
        duckdb_connection = setup_duckdb()
        if duckdb_connection is not None:
            app.state.duckdb_con = duckdb_connection
            print("‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö DuckDB –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
        else:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö")
            app.state.duckdb_con = None
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö: {e}")
        app.state.duckdb_con = None

@app.on_event("shutdown")
async def shutdown_event():
    """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤ –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏"""
    print("üõë –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã Text2SQL Multi-Agent System...")

    try:
        if (hasattr(app.state, 'duckdb_con') and
            app.state.duckdb_con is not None and
            hasattr(app.state.duckdb_con, 'close')):
            app.state.duckdb_con.close()
            print("‚úÖ –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö –∑–∞–∫—Ä—ã—Ç–æ")
        else:
            print("‚ÑπÔ∏è –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö —É–∂–µ –∑–∞–∫—Ä—ã—Ç–æ –∏–ª–∏ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ")
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–∫—Ä—ã—Ç–∏–∏ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è: {e}")

@app.route('/visualize', methods=['POST'])
def visualize():
    try:
        data = request.get_json()
        df = pd.DataFrame(data['result'])
        status, content = visualize_report(df)
        return jsonify({'status': status, 'content': content})
    except Exception as e:
        return jsonify({'status': 'error', 'content': str(e)})

# ==============================================================================
# –ó–ê–ü–£–°–ö –°–ï–†–í–ï–†–ê
# ==============================================================================

if __name__ == "__main__":
    print("üöÄ –ó–∞–ø—É—Å–∫ Text2SQL Multi-Agent System with LLM Analysis...")

    import uvicorn
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=5002,
        reload=False,
        log_level="info",
        access_log=True
    )
