## anomaly_agent.py
import numpy as np
import pandas as pd
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from scipy import stats
import json
import warnings
from langchain_core.messages import HumanMessage

warnings.filterwarnings('ignore')

class AnomalyDetectorAgent:
    """–ê–≥–µ–Ω—Ç –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π –≤ –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self, llm_client, verbose=True):
        self.llm_client = llm_client
        self.name = "AnomalyDetector"
        self.role = "–°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –ø–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—é –∞–Ω–æ–º–∞–ª–∏–π"
        self.verbose = verbose
        
    def call_giga_api_wrapper(self, prompt_text, system_instruction):
        """–û–±–µ—Ä—Ç–∫–∞ –¥–ª—è –≤—ã–∑–æ–≤–∞ GigaChat API"""
        if self.llm_client is None:
            return "[–û–®–ò–ë–ö–ê API] –ö–ª–∏–µ–Ω—Ç GigaChat –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω."
        
        try:
            from langchain_core.messages import SystemMessage
            messages = [SystemMessage(content=system_instruction), HumanMessage(content=prompt_text)]
            res = self.llm_client.invoke(messages)
            return res.content.strip()
        except Exception as e:
            if self.verbose:
                print(f"–û—à–∏–±–∫–∞ –≤—ã–∑–æ–≤–∞ GigaChat API –≤ –∞–≥–µ–Ω—Ç–µ –∞–Ω–æ–º–∞–ª–∏–π: {e}")
            return f"[–û–®–ò–ë–ö–ê API] {e}"
    
    def detect_anomalies(self, sql_result_df, user_prompt):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π"""
        
        if sql_result_df is None or sql_result_df.empty:
            return {
                "anomalies_found": False,
                "message": "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∞–Ω–æ–º–∞–ª–∏–π",
                "details": {},
                "agent": self.name
            }
        
        if self.verbose:
            print(f"\nüîç {self.name}: –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é {len(sql_result_df)} –∑–∞–ø–∏—Å–µ–π...")
        
        # –í—ã–±–∏—Ä–∞–µ–º —á–∏—Å–ª–æ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã
        numeric_columns = sql_result_df.select_dtypes(include=[np.number]).columns.tolist()
        
        if not numeric_columns:
            return {
                "anomalies_found": False,
                "message": "–ù–µ—Ç —á–∏—Å–ª–æ–≤—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∞–Ω–æ–º–∞–ª–∏–π",
                "details": {},
                "agent": self.name
            }
        
        if self.verbose:
            print(f"üìä –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —á–∏—Å–ª–æ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã: {numeric_columns}")
        
        anomaly_results = {}
        
        try:
            # –ú–µ—Ç–æ–¥ 1: Z-Score –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∏—Å–ª–æ–≤–æ–≥–æ —Å—Ç–æ–ª–±—Ü–∞
            for col in numeric_columns:
                if sql_result_df[col].isnull().all():
                    continue
                    
                z_scores = np.abs(stats.zscore(sql_result_df[col].dropna()))
                threshold = 3.0
                anomaly_indices = np.where(z_scores > threshold)
                
                anomaly_results[col] = {
                    "method": "Z-Score",
                    "threshold": threshold,
                    "anomaly_count": len(anomaly_indices),
                    "anomaly_percentage": round((len(anomaly_indices) / len(sql_result_df)) * 100, 2),
                    "max_z_score": float(np.max(z_scores)) if len(z_scores) > 0 else 0,
                    "anomaly_indices": anomaly_indices.tolist()[:5]  # –ü–µ—Ä–≤—ã–µ 5 –∏–Ω–¥–µ–∫—Å–æ–≤
                }
            
            # –ú–µ—Ç–æ–¥ 2: IQR –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤
            for col in numeric_columns:
                if sql_result_df[col].isnull().all():
                    continue
                
                Q1 = sql_result_df[col].quantile(0.25)
                Q3 = sql_result_df[col].quantile(0.75)
                IQR = Q3 - Q1
                
                if IQR > 0:  # –ò–∑–±–µ–≥–∞–µ–º –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å
                    lower_bound = Q1 - 1.5 * IQR
                    upper_bound = Q3 + 1.5 * IQR
                    
                    anomaly_mask = (sql_result_df[col] < lower_bound) | (sql_result_df[col] > upper_bound)
                    anomaly_count = anomaly_mask.sum()
                    
                    anomaly_results[f"{col}_IQR"] = {
                        "method": "IQR",
                        "Q1": float(Q1),
                        "Q3": float(Q3),
                        "IQR": float(IQR),
                        "lower_bound": float(lower_bound),
                        "upper_bound": float(upper_bound),
                        "anomaly_count": int(anomaly_count),
                        "anomaly_percentage": round((anomaly_count / len(sql_result_df)) * 100, 2)
                    }
            
            # –ú–µ—Ç–æ–¥ 3: Isolation Forest (–µ—Å–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö)
            if len(sql_result_df) >= 20 and len(numeric_columns) >= 1:
                try:
                    df_clean = sql_result_df[numeric_columns].dropna()
                    if len(df_clean) >= 20:
                        scaler = StandardScaler()
                        data_scaled = scaler.fit_transform(df_clean)
                        
                        iso_forest = IsolationForest(contamination=0.1, random_state=42)
                        predictions = iso_forest.fit_predict(data_scaled)
                        
                        anomaly_count = (predictions == -1).sum()
                        
                        anomaly_results["isolation_forest"] = {
                            "method": "Isolation Forest",
                            "contamination": 0.1,
                            "anomaly_count": int(anomaly_count),
                            "anomaly_percentage": round((anomaly_count / len(df_clean)) * 100, 2),
                            "analyzed_records": len(df_clean)
                        }
                except Exception as e:
                    if self.verbose:
                        print(f"–û—à–∏–±–∫–∞ Isolation Forest: {e}")
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—é —á–µ—Ä–µ–∑ LLM
            interpretation = self._generate_interpretation(anomaly_results, sql_result_df, user_prompt)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –Ω–∞–π–¥–µ–Ω—ã –ª–∏ –∑–Ω–∞—á–∏–º—ã–µ –∞–Ω–æ–º–∞–ª–∏–∏
            significant_anomalies = any(
                result.get("anomaly_percentage", 0) > 5 
                for result in anomaly_results.values() 
                if isinstance(result, dict) and "anomaly_percentage" in result
            )
            
            return {
                "anomalies_found": significant_anomalies,
                "message": interpretation,
                "details": anomaly_results,
                "analyzed_columns": numeric_columns,
                "total_records": len(sql_result_df),
                "agent": self.name
            }
            
        except Exception as e:
            if self.verbose:
                print(f"–û—à–∏–±–∫–∞ –≤ –∞–≥–µ–Ω—Ç–µ –∞–Ω–∞–ª–∏–∑–∞ –∞–Ω–æ–º–∞–ª–∏–π: {e}")
            return {
                "anomalies_found": False,
                "message": f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –∞–Ω–æ–º–∞–ª–∏–π: {str(e)}",
                "details": {},
                "agent": self.name
            }
    
    def _generate_interpretation(self, anomaly_results, data, user_prompt):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ –∞–Ω–æ–º–∞–ª–∏–π"""
        
        # –£–ø—Ä–æ—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞
        simplified_results = {}
        for key, value in anomaly_results.items():
            if isinstance(value, dict) and "anomaly_percentage" in value:
                simplified_results[key] = {
                    "method": value.get("method", key),
                    "anomaly_count": value["anomaly_count"],
                    "anomaly_percentage": value["anomaly_percentage"]
                }
        
        prompt = f"""
        –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π –≤ –¥–∞–Ω–Ω—ã—Ö –∏ –¥–∞–π –∫—Ä–∞—Ç–∫–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.
        
        –ò—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {user_prompt}
        –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {len(data)}
        
        –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –∞–Ω–æ–º–∞–ª–∏–π:
        {json.dumps(simplified_results, ensure_ascii=False, indent=2)}
        
        –°–æ–∑–¥–∞–π –∫—Ä–∞—Ç–∫–∏–π –æ—Ç—á–µ—Ç (–º–∞–∫—Å–∏–º—É–º 150 —Å–ª–æ–≤), –≤–∫–ª—é—á–∞—é—â–∏–π:
        1. –û–±—â—É—é –æ—Ü–µ–Ω–∫—É –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
        2. –ù–∞–π–¥–µ–Ω–Ω—ã–µ –∞–Ω–æ–º–∞–ª–∏–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
        3. –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã –∞–Ω–æ–º–∞–ª–∏–π
        4. –ö—Ä–∞—Ç–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        
        –û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –±–µ–∑ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –¥–µ—Ç–∞–ª–µ–π.
        """
        
        try:
            return self.call_giga_api_wrapper(prompt, "–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –¥–∞–Ω–Ω—ã—Ö, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—â–∏–π—Å—è –Ω–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–∏ –∞–Ω–æ–º–∞–ª–∏–π.")
        except Exception as e:
            if self.verbose:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –∞–Ω–æ–º–∞–ª–∏–π: {e}")
            
            # –ü—Ä–æ—Å—Ç–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –±–µ–∑ LLM
            total_anomalies = sum(
                result.get("anomaly_count", 0) 
                for result in simplified_results.values()
            )
            
            if total_anomalies > 0:
                return f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {total_anomalies} –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –∞–Ω–æ–º–∞–ª–∏–π –≤ –¥–∞–Ω–Ω—ã—Ö. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –≤—ã—è–≤–ª–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π."
            else:
                return "–ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö –∞–Ω–æ–º–∞–ª–∏–π –≤ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ. –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ–∂–∏–¥–∞–µ–º—ã–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º."

# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –∞–Ω–∞–ª–∏–∑–∞ –∞–Ω–æ–º–∞–ª–∏–π
def should_analyze_anomalies_by_data(sql_results_df, sql_query, config):
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –∞–Ω–∞–ª–∏–∑–∞ –∞–Ω–æ–º–∞–ª–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –¥–∞–Ω–Ω—ã—Ö"""
    
    if sql_results_df is None or sql_results_df.empty:
        print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        return False
    
    criteria = {
        'has_numeric_data': len(sql_results_df.select_dtypes(include=[np.number]).columns) >= config["min_numeric_columns"],
        'sufficient_data': len(sql_results_df) >= config["min_records"],
        'multiple_regions': (
            'territory_id' in sql_results_df.columns and 
            sql_results_df['territory_id'].nunique() >= config["min_regions"]
        ) if config["min_regions"] > 0 else False,
        'salary_data': (
            config["enable_for_salary"] and (
                'salary' in sql_query.lower() or 
                any('value' in col.lower() for col in sql_results_df.columns) or
                's.value' in sql_query.lower()
            )
        ),
        'aggregated_data': (
            config["enable_for_aggregations"] and 
            any(keyword in sql_query.upper() for keyword in ['AVG', 'SUM', 'COUNT', 'MAX', 'MIN'])
        ),
        'time_series': (
            config["enable_for_time_series"] and 
            any(col in sql_results_df.columns for col in ['year', 'period', 'date'])
        )
    }
    
    print(f"üìã –ü–æ–¥—Ä–æ–±–Ω—ã–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö:")
    for key, value in criteria.items():
        print(f"   {key}: {value}")
    
    criteria_met = sum(criteria.values())
    threshold = config.get("auto_threshold", 2)
    
    print(f"üìä –ö—Ä–∏—Ç–µ—Ä–∏–µ–≤ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ: {criteria_met} –∏–∑ {threshold} —Ç—Ä–µ–±—É–µ–º—ã—Ö")
    
    return criteria_met >= threshold
